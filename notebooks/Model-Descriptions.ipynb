{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Timothy Hochberg <tim@skytruth.org>, Egil MÃ¶ller <egil@skytruth.org>\n",
    "\n",
    "Model Descriptions\n",
    "------------------\n",
    "\n",
    "\n",
    "## Heuristic Model\n",
    "\n",
    "The first model developed is referred to as the\n",
    "*heuristic model* and was derived by observing that there were\n",
    "correlations between fishing behaviour and several of the\n",
    "values present in AIS messages. In particular, the\n",
    "likelihood that a vessel was fishing tends to increase with\n",
    "the standard deviation of the speed and course, but to decrease\n",
    "with mean speed. These features were used to develop the \n",
    "*heursitic model*:\n",
    "\n",
    "$$\n",
    "fishing\\_score = \\frac{2}{3}\\left(\\sigma_{s_m} + \\sigma_{c_m} + \\overline{s_m}\\right) \n",
    "$$\n",
    "\n",
    "Here $s_m$ and $c_m$ are simple features derived from the\n",
    "speed and course respectively and $\\sigma_x$ and $\\overline{x}$\n",
    "have their standard meanings\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "s_m & \\equiv 1.0 - \\min\\left(1, speed\\,/\\,17\\right) \\\\\n",
    "c_m & \\equiv course\\,/\\,360 \\\\\n",
    "\\sigma_x & \\equiv \\text{standard deviation of } x \\\\\n",
    "\\overline{x} & \\equiv \\text{mean of } x\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For the *heuristic model*, the means and standard deviations are computed \n",
    "over a one hour window.\n",
    "\n",
    "The heuristic model performs reasonably well trawlers and\n",
    "longliners, but poorly for purse seiners.\n",
    "\n",
    "  * Implementation: https://github.com/GlobalFishingWatch/vessel-scoring/blob/release-1.0/vessel_scoring/legacy_heuristic_model.py\n",
    "  * Normalization and calculation of $\\sigma_{s_m}$, $\\sigma_{c_m}$, $\\overline{s_m}$\n",
    "    (note that $s_m$ and $c_m$ are referred to as `measure_speed` and `measure_course`\n",
    "    in the code): \n",
    "    https://github.com/GlobalFishingWatch/vessel-scoring/blob/release-1.0/vessel_scoring/add_measures.py\n",
    "\n",
    "\n",
    "## Generic Model\n",
    "\n",
    "A series of logistic regression models were then developed\n",
    "using the same three features found in the *heursitic\n",
    "model*. In order to increase the expressiveness of the\n",
    "logistic model, powers of the 3 base features are added to\n",
    "the features. Thus, the full feature vector consists of:\n",
    "$$\n",
    "\\sigma_{s_m}, \\sigma_{s_m}^2,\\ldots, \\sigma_{s_m}^n, \\sigma_{c_m}, \\sigma_{c_m}^2,\\ldots, \\sigma_{c_m}^n,\n",
    "\\overline{s_m},\\overline{s_m}^2, \\ldots \\overline{s_m}^n \\\\\n",
    "$$\n",
    "where $n$ is what we shall be refer to as the *feature order*.\n",
    "Note that that despite the odd form of\n",
    "$s_m$, from the point of view of the\n",
    "logistic model, it is equivalent to the the speed\n",
    "capped at 17 knots.\n",
    "\n",
    "The first of the logistic models, referred to as the\n",
    "*generic model*, is the model currently in use and \n",
    "is a logistic model using a 12 hour time window\n",
    "and a feature order of 6. One model is trained for all gear\n",
    "types. This model generally performs bettter than the\n",
    "heuristic model, but still performs rather poorly on purse\n",
    "seiners. The 12-hour window was arrived at by plotting the model\n",
    "accuracy versus window size. There is a different optimal\n",
    "window size for each gear type, but 12 hours performed\n",
    "well for a model trained and tested on all gear types.\n",
    "\n",
    "  * Implementation: https://github.com/GlobalFishingWatch/vessel-scoring/blob/release-1.0/vessel_scoring/logistic_model.py\n",
    "  * Parameters for the implementation: https://github.com/GlobalFishingWatch/vessel-scoring/blob/release-1.0/vessel_scoring/models.py#L17\n",
    "  * Normalization and calculation of $\\sigma_{s_m}$, $\\sigma_{c_m}$, $\\overline{s_m}$\n",
    "    (note that $s_m$ and $c_m$ are referred to as `measure_speed` and `measure_course`\n",
    "    in the code): https://github.com/GlobalFishingWatch/vessel-scoring/blob/release-1.0/vessel_scoring/add_measures.py\n",
    "\n",
    "\n",
    "## Multi-Window Model\n",
    "\n",
    "The multi-window model \n",
    "is a logistic model similar to the *generic model* except that\n",
    "it uses\n",
    "multiple time windows, ranging in duration from one-half to\n",
    "twenty four hours. Using multiple window sizes both provides\n",
    "a richer feature set and avoids the needs to optimize over\n",
    "window size. \n",
    "\n",
    "\n",
    "* Implementation: https://github.com/GlobalFishingWatch/vessel-scoring/blob/release-1.1/vessel_scoring/logistic_model.py\n",
    "* Parameters for the implementation: https://github.com/GlobalFishingWatch/vessel-scoring/blob/release-1.1/vessel_scoring/models.py#L20\n",
    "  * Windows used: https://github.com/GlobalFishingWatch/vessel-scoring/blob/release-1.1/vessel_scoring/colspec.py#L4\n",
    "\n",
    "## Multi-Window, Gear-Types-Specific Models\n",
    "\n",
    "The multi-window gear-type-specific model, which \n",
    "is on the verge of being deployed, are a set of \n",
    "models, each the same as the Multi-Window model, \n",
    "but each trained on only vessels with a specific \n",
    "gear type. We have currently trained the model \n",
    "for longliners, trawlers and purse seiners. \n",
    "We are also experimenting with adding other features.\n",
    "In particular, whether it is currently daylight appears to\n",
    "be a very useful feature for predicting purse seine fishing.\n",
    "These changes, taken together, dramatically improve the\n",
    "performance, particularly of purse seiners.\n",
    "\n",
    "* Implementation: https://github.com/GlobalFishingWatch/vessel-scoring/blob/release-1.1/vessel_scoring/logistic_model.py\n",
    "* Parameters for the implementation: https://github.com/GlobalFishingWatch/vessel-scoring/blob/release-1.1/vessel_scoring/models.py#L22-L38\n",
    "\n",
    "## Future Models\n",
    "\n",
    "It is straightforward to use the multi-window logistic\n",
    "model features described above with a random forest or neural net\n",
    "model. In early experiments, both of these model types offer\n",
    "slightly improved performance relative to logistic model while at\n",
    "the same eliminating the need to augment the feature vector\n",
    "with powers of the base features.\n",
    "\n",
    "We eventually plan to experiment with using convolutional or\n",
    "recurrent neural networks to find features in the AIS data\n",
    "directly rather than hand engineering the features.\n",
    "\n",
    "\n",
    "General Notes\n",
    "-------------\n",
    "\n",
    "The precision of the models vary by gear type: Long liners are easiest to\n",
    "predict, even for a model trained on all gear types,\n",
    "followed by trawlers; purse seiners are the worst.\n",
    "\n",
    "We have evaluated the models using a separate test set (and\n",
    "for window size and feature order, optimization, using\n",
    "separate train-, validation- and test-sets) plotting\n",
    "precision/recall and ROC curves.\n",
    "\n",
    "We have also evaluated the generic model on each gear type\n",
    "separately as well as on the combined data set. In addition,\n",
    "for longliners we have cross trained and validated between\n",
    "two separately labelled datasets with slightly different\n",
    "labeling methods (Kristinas' and Alex data).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import vessel_scoring.models\n",
    "from vessel_scoring.models import train_model_on_data\n",
    "from vessel_scoring import data, utils\n",
    "from vessel_scoring.evaluate_model import evaluate_model, compare_models\n",
    "from IPython.core.display import display, HTML, Markdown\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "\n",
    "from numpy.lib.recfunctions import append_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_cls(x):\n",
    "    return np.array(append_fields(x, 'classification', np.random.uniform(0, 1, x.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_cls_file(i, o):\n",
    "    np.savez(o, x=add_cls(np.load(i)['x']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and test data\n",
    "for s in enumerate(['datasets/kristina_longliner.measures.npz', 'datasets/kristina_trawl.measures.npz',\n",
    "                    'datasets/kristina_ps.measures.npz']):\n",
    "    add_cls_file(s[1], str(s[0]))\n",
    "    print(\"done\" + str(s[0]))\n",
    "    \n",
    "# Data supplied by Kristina\n",
    "_, train_lline,  valid_lline, test_lline = data.load_dataset_by_vessel(\n",
    "        '0.npz')\n",
    "print(\"done\")\n",
    "_, train_trawl,  valid_trawl, test_trawl = data.load_dataset_by_vessel(\n",
    "        '1.npz')\n",
    "print(\"done\")\n",
    "_, train_pseine, valid_pseine, test_pseine = data.load_dataset_by_vessel(\n",
    "        '2.npz')\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load training and test data\n",
    "for s in enumerate(['datasets/classified-filtered.measures.npz',\n",
    "                    'datasets/slow-transits.measures.npz']):\n",
    "    add_cls_file(s[1], str(s[0]))\n",
    "    print(\"done\" + str(s[0]))\n",
    "    \n",
    "# Data supplied by Kristina\n",
    "# _, train_lline,  valid_lline, test_lline = data.load_dataset_by_vessel(\n",
    "#         '0.npz')\n",
    "# print(\"done\")\n",
    "# _, train_trawl,  valid_trawl, test_trawl = data.load_dataset_by_vessel(\n",
    "#         '1.npz')\n",
    "# print(\"done\")\n",
    "# _, train_pseine, valid_pseine, test_pseine = data.load_dataset_by_vessel(\n",
    "#         '2.npz')\n",
    "# print(\"done\")\n",
    "# Crowd sourced longliner data\n",
    "test_lline_crowd, _, _, _ = data.load_dataset_by_vessel(\n",
    "        \"0.npz\")\n",
    "print(\"done\")\n",
    "\n",
    "# Slow transits (used to train models to avoid classifying slow transits as fishing)\n",
    "TRANSIT_WEIGHT = 10\n",
    "x_tran, xtrain_tran, xcross_tran, xtest_tran = data.load_dataset_by_vessel(\n",
    "                                    '1.npz', even_split=False)\n",
    "print(\"done\")\n",
    "xtrain_tran = utils.clone_subset(xtrain_tran, test_lline.dtype)\n",
    "xcross_tran = utils.clone_subset(xcross_tran, test_lline.dtype)\n",
    "xtest_tran = utils.clone_subset(xtest_tran, test_lline.dtype)\n",
    "train_tran = np.concatenate([xtrain_tran, xcross_tran] * TRANSIT_WEIGHT)\n",
    "\n",
    "train = np.concatenate([train_trawl, train_lline, train_pseine, \n",
    "                        valid_lline, valid_trawl, valid_pseine, train_tran])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How much test data do we have\n",
    "\n",
    "Our initial test and training data consisted of roughly a dozen different vessels of each type \n",
    "classified over a multi-year period by Kristina Boerder of Dalhousie University. One-quarter of \n",
    "those are used for testing, so there is a relatively small number of different vessels in the test\n",
    "sets. \n",
    "\n",
    "In addition, we are beginning to collect crowd sourced data for both testing and training. Some of the\n",
    "early crowd sourced data, available for long liners only, is used as an additional test set in the examples\n",
    "below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name, test_data in [(\"trawlers\", test_trawl),\n",
    "                        (\"purse seiners\", test_pseine),\n",
    "                        (\"longliners\", test_lline),\n",
    "                        (\"crowd sourced longliners\", test_lline_crowd)]:\n",
    "    mmsi_count = len(set(test_data['mmsi']))\n",
    "    pt_count = len(test_data)\n",
    "    print(\"For {0} we have {1} test vessels with {2} test points\".format(name, mmsi_count, pt_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare the models\n",
    "\n",
    "from vessel_scoring.legacy_heuristic_model import LegacyHeuristicModel\n",
    "from vessel_scoring.logistic_model import LogisticModel\n",
    "\n",
    "uniform_training_data = {'longliner': train, \n",
    "                         'longliner crowd' : train,\n",
    "                         'trawler': train, \n",
    "                         'purse_seine': train}\n",
    "\n",
    "gear_specific_training_data = {'longliner': np.concatenate([train_lline, valid_lline, train_tran]), \n",
    "                               'longliner crowd' : np.concatenate([train_lline, valid_lline, train_tran]), \n",
    "                               'trawler': np.concatenate([train_trawl, valid_trawl, train_tran]),\n",
    "                               'purse_seine': np.concatenate([train_pseine, valid_pseine, train_tran])}\n",
    "\n",
    "test_data = {'longliner': test_lline, \n",
    "             'longliner crowd': test_lline_crowd,\n",
    "             'trawler': test_trawl, \n",
    "             'purse_seine': test_pseine}\n",
    "\n",
    "untrained_models = [\n",
    "    (\"Heuristic Model\", LegacyHeuristicModel(window=3600), \n",
    "         uniform_training_data),\n",
    "    ('Generic Model', LogisticModel(colspec=dict(windows=[43200]), order=6),\n",
    "         uniform_training_data),\n",
    "    ('Multi-Window, Gear-Type-Specific Models', LogisticModel(colspec=dict(\n",
    "        windows=[1800, 3600, 10800, 21600, 43200, 86400],\n",
    "        measures=['measure_daylight', 'measure_speed']), order=6), \n",
    "         gear_specific_training_data),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparisons\n",
    "\n",
    "The models output a numbers between 0 and 1 that correspond to how \n",
    "confident they are that there is fishing occuring. For\n",
    "the first set of comparisons we treat predictions `>0.5`\n",
    "as fishing and those `<=0.5` as nonfishing. This allows us to use\n",
    "*precision*, *recall* and *f1-score* as metrics. We also show Receiver\n",
    "Operator Characteristic (ROC) area under the curve (AUC) plots and \n",
    "precision recall plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for gear in ['purse_seine', 'trawler', 'longliner', 'longliner crowd']:\n",
    "    X_test = test_data[gear]\n",
    "\n",
    "    display(HTML(\"<h2>{}</h2>\".format(gear.replace('_', ' ').title())))\n",
    "        \n",
    "    trained_models = [(name, train_model_on_data(mdl, X_train[gear])) for (name, mdl, X_train) in untrained_models]\n",
    "    \n",
    "    predictions = []\n",
    "    for name, mdl in trained_models:\n",
    "        predictions.append((name, (mdl.predict_proba(X_test)[:,1] > 0.5), X_test['classification'] > 0.5))\n",
    "\n",
    "    lines = [\"|Model|Recall|Precision|F1-Score|\",\n",
    "         \"|-----|------|---------|--------|\"]\n",
    "    for name, pred, actual in predictions:\n",
    "        lines.append(\"|{}|{:.2f}|{:.2f}|{:.2f}|\".format(name, \n",
    "                                            metrics.recall_score(actual, pred),\n",
    "                                            metrics.precision_score(actual, pred), \n",
    "                                            metrics.f1_score(actual, pred)))\n",
    "\n",
    "    display(Markdown('\\n'.join(lines)))\n",
    "\n",
    "    compare_models(trained_models, X_test)\n",
    "    \n",
    "    display(HTML(\"<hr/>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
