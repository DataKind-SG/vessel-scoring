{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Models\n",
    "\n",
    "This notebook compares various GFW models based on the `measure_speed` and `measure_course` with each other\n",
    "and with the models from Dalhousie University.  Note that the distance-to-shore cutoff was disabled in the\n",
    "Dalhousie models, so none of the models compared here are using distance-to-shore as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "%matplotlib inline\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append('..')\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "from IPython.core.display import display, HTML, Markdown\n",
    "import datetime\n",
    "import pytz\n",
    "from sklearn import metrics\n",
    "\n",
    "import vessel_scoring.models\n",
    "import vessel_scoring.evaluate_model\n",
    "from vessel_scoring import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning, insufficient items to sample, returning all\n",
      "Warning, insufficient items to sample, returning all\n"
     ]
    }
   ],
   "source": [
    "data = vessel_scoring.models.load_data('../datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../datasets/from_ranges/split_Longliners.csv.npz', '../datasets/from_ranges/split_Longliners.measures.npz', '../datasets/from_ranges/split_Pole_and_Line.csv.npz', '../datasets/from_ranges/split_Pole_and_Line.measures.npz', '../datasets/from_ranges/split_Pots_and_Traps.csv.npz', '../datasets/from_ranges/split_Pots_and_Traps.measures.npz', '../datasets/from_ranges/split_Purse_seines.csv.npz', '../datasets/from_ranges/split_Purse_seines.measures.npz', '../datasets/from_ranges/split_Set_gillnets.csv.npz', '../datasets/from_ranges/split_Set_gillnets.measures.npz', '../datasets/from_ranges/split_Trawlers.csv.npz', '../datasets/from_ranges/split_Trawlers.measures.npz', '../datasets/from_ranges/split_Trollers.csv.npz', '../datasets/from_ranges/split_Trollers.measures.npz']\n",
      "Warning, insufficient items to sample, returning all\n",
      "Warning, insufficient items to sample, returning all\n",
      "Warning, insufficient items to sample, returning all\n",
      "Warning, insufficient items to sample, returning all\n"
     ]
    }
   ],
   "source": [
    "import vessel_scoring.data\n",
    "from glob import glob\n",
    "import os\n",
    "paths = glob(\"../datasets/from_ranges/*.npz\")\n",
    "print(paths)\n",
    "rngdata = {os.path.basename(p) : vessel_scoring.data.load_dataset_by_vessel(p) for p in paths\n",
    "          if p.endswith('.measures.npz')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_Pots_and_Traps.measures.npz\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'kristina_split_Pots_and_Traps.measures.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-16cf4a528bfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0morig_mmsi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"kristina_{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mmsi'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mrngdata_mmsi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrngdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgear\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mmsi'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'kristina_split_Pots_and_Traps.measures.npz'"
     ]
    }
   ],
   "source": [
    "for gear in rngdata:\n",
    "    print(gear)\n",
    "\n",
    "    orig_mmsi = sorted(set(data[\"kristina_{}\".format(gear)]['all']['mmsi']))\n",
    "\n",
    "    rngdata_mmsi = sorted(set(rngdata[gear]['mmsi']))\n",
    "\n",
    "    common = sorted(set(orig_mmsi) & set(rngdata_mmsi))\n",
    "\n",
    "    def plot(x, ax):\n",
    "        mask = utils.is_fishy(x)\n",
    "        ax.plot(x[~mask]['lon'], x[~mask]['lat'], 'b.')\n",
    "        ax.plot(x[mask]['lon'], x[mask]['lat'], 'r.')\n",
    "\n",
    "\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    def plot_stuff(n):\n",
    "        mmsi = common[n]\n",
    "        rngd = rngdata[gear][rngdata[gear]['mmsi'] == mmsi]\n",
    "        origd = data[\"kristina_{}\".format(gear)]['all'][data[\"kristina_{}\".format(gear)]['all']['mmsi'] == mmsi]\n",
    "        fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(16, 4))\n",
    "        plot(origd, ax1)\n",
    "        plot(rngd, ax2)\n",
    "        ax2.set_xlim(*ax1.get_xlim())\n",
    "        ax2.set_ylim(*ax1.get_ylim())\n",
    "        plt.show()\n",
    "        print(len(rngd) / len(origd) if len(origd) else None, len(rngd), len(origd))\n",
    "\n",
    "\n",
    "    for i in range(len(common)):\n",
    "        plot_stuff(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Longliners': 0.86721601765,\n",
       " 'Pole and Line': 0.953205472904,\n",
       " 'Pots and Traps': 0.984735367311,\n",
       " 'Purse seines': 0.916769633641,\n",
       " 'Set gillnets': 0.965576816174,\n",
       " 'Trawlers': 0.932607775748,\n",
       " 'Trollers': 0.946113904902}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Pairwise agreement for Longliners 0.86721601765\n",
    "Consensus agreement for Longliners 0.933060883222\n",
    "Standard Deviationn for Longliners 0.0824210430256\n",
    "\n",
    "Pairwise agreement for Pole and Line 0.953205472904\n",
    "Consensus agreement for Pole and Line 0.975634821055\n",
    "Standard Deviationn for Pole and Line 0.029320597482\n",
    "\n",
    "Pairwise agreement for Pots and Traps 0.984735367311\n",
    "Consensus agreement for Pots and Traps 0.992367683656\n",
    "Standard Deviationn for Pots and Traps 0.0101241072179\n",
    "\n",
    "Pairwise agreement for Purse seines 0.916769633641\n",
    "Consensus agreement for Purse seines 0.958270409071\n",
    "Standard Deviationn for Purse seines 0.0513507211783\n",
    "\n",
    "Pairwise agreement for Set gillnets 0.965576816174\n",
    "Consensus agreement for Set gillnets 0.982649433863\n",
    "Standard Deviationn for Set gillnets 0.0225498520952\n",
    "\n",
    "Pairwise agreement for Trawlers 0.932607775748\n",
    "Consensus agreement for Trawlers 0.966164556048\n",
    "Standard Deviationn for Trawlers 0.0418758939906\n",
    "\n",
    "Pairwise agreement for Trollers 0.946113904902\n",
    "Consensus agreement for Trollers 0.973056952451\n",
    "Standard Deviationn for Trollers 0.0319558123579\"\"\"\n",
    "\n",
    "agreement = {}\n",
    "primer = \"Pairwise agreement for \"\n",
    "for line in text.split(\"\\n\"):\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    if line.startswith(primer):\n",
    "        line = line[len(primer):]\n",
    "        name, strval = line.rsplit(\" \", 1)\n",
    "        val = float(strval)\n",
    "        agreement[name] = val\n",
    "        \n",
    "agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fishing_localization = \"\"\"\n",
    "<div class=\"unbreakable\">\n",
    "  <table>\n",
    "    <tr>\n",
    "      <th>\n",
    "        Gear Type\n",
    "      </th>\n",
    "      <th>\n",
    "        Precision\n",
    "      </th>\n",
    "      <th>\n",
    "        Recall\n",
    "      </th>\n",
    "      <th>\n",
    "        Accuracy\n",
    "      </th>\n",
    "      <th>\n",
    "        F1-Score\n",
    "      </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>\n",
    "        Longliners\n",
    "      </td>\n",
    "      <td>\n",
    "        0.85\n",
    "      </td>\n",
    "      <td>\n",
    "        0.82\n",
    "      </td>\n",
    "      <td>\n",
    "        0.85\n",
    "      </td>\n",
    "      <td>\n",
    "        0.83\n",
    "      </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>\n",
    "        Pole and Line\n",
    "      </td>\n",
    "      <td>\n",
    "        0.91\n",
    "      </td>\n",
    "      <td>\n",
    "        0.95\n",
    "      </td>\n",
    "      <td>\n",
    "        0.96\n",
    "      </td>\n",
    "      <td>\n",
    "        0.93\n",
    "      </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>\n",
    "        Pots and Traps\n",
    "      </td>\n",
    "      <td>\n",
    "        0.98\n",
    "      </td>\n",
    "      <td>\n",
    "        0.97\n",
    "      </td>\n",
    "      <td>\n",
    "        0.98\n",
    "      </td>\n",
    "      <td>\n",
    "        0.97\n",
    "      </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>\n",
    "        Purse seines\n",
    "      </td>\n",
    "      <td>\n",
    "        0.83\n",
    "      </td>\n",
    "      <td>\n",
    "        0.87\n",
    "      </td>\n",
    "      <td>\n",
    "        0.90\n",
    "      </td>\n",
    "      <td>\n",
    "        0.85\n",
    "      </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>\n",
    "        Set gillnets\n",
    "      </td>\n",
    "      <td>\n",
    "        0.97\n",
    "      </td>\n",
    "      <td>\n",
    "        0.86\n",
    "      </td>\n",
    "      <td>\n",
    "        0.93\n",
    "      </td>\n",
    "      <td>\n",
    "        0.91\n",
    "      </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>\n",
    "        Trawlers\n",
    "      </td>\n",
    "      <td>\n",
    "        0.94\n",
    "      </td>\n",
    "      <td>\n",
    "        0.72\n",
    "      </td>\n",
    "      <td>\n",
    "        0.87\n",
    "      </td>\n",
    "      <td>\n",
    "        0.81\n",
    "      </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>\n",
    "        Trollers\n",
    "      </td>\n",
    "      <td>\n",
    "        0.94\n",
    "      </td>\n",
    "      <td>\n",
    "        0.52\n",
    "      </td>\n",
    "      <td>\n",
    "        0.74\n",
    "      </td>\n",
    "      <td>\n",
    "        0.67\n",
    "      </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>\n",
    "        Overall\n",
    "      </td>\n",
    "      <td>\n",
    "        0.88\n",
    "      </td>\n",
    "      <td>\n",
    "        0.80\n",
    "      </td>\n",
    "      <td>\n",
    "        0.87\n",
    "      </td>\n",
    "      <td>\n",
    "        0.84\n",
    "      </td>\n",
    "    </tr>\n",
    "  </table>\n",
    "</div>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Comparison function:\n",
    "#   1. Convert output to ranges\n",
    "#   2. Convert by minutes\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../mussidae\")\n",
    "import mussidae.time_range_tools as trtools\n",
    "sys.path.append(\"../../vessel-classification-pipeline/classification\")\n",
    "from  classification.utility import is_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compare_fishing_localization(true_ranges, inferred_ranges):\n",
    "\n",
    "    all_mmsi = sorted(set(x.mmsi for x in true_ranges))\n",
    "\n",
    "    true_by_mmsi = {}\n",
    "    pred_by_mmsi = {}\n",
    "\n",
    "    for mmsi in all_mmsi:\n",
    "        true = np.array([x for x in true_ranges if x.mmsi == mmsi])\n",
    "        inferred = np.array([x for x in inferred_ranges if x.mmsi == mmsi])\n",
    "\n",
    "        # Determine minutes from start to finish of this mmsi, create an array to\n",
    "        # hold results and fill with -1 (unknown)\n",
    "        _, start, end, _ = true[0]\n",
    "        for (_, s, e, _) in true[1:]:\n",
    "            start = min(start, s)\n",
    "            end = max(end, e)\n",
    "        start_min = datetime_to_minute(start)\n",
    "        end_min = datetime_to_minute(end)\n",
    "        \n",
    "        minutes = np.empty([end_min - start_min + 1, 2], dtype=int)\n",
    "        minutes.fill(-1)\n",
    "\n",
    "        # Fill in minutes[:, 0] with known true / false values\n",
    "        for (_, s, e, is_fishing) in true:\n",
    "            s_min = datetime_to_minute(s)\n",
    "            e_min = datetime_to_minute(e)\n",
    "            for m in range(s_min - start_min, e_min - start_min + 1):\n",
    "                minutes[m, 0] = is_fishing\n",
    "\n",
    "        # fill in minutes[:, 1] with in inferred values\n",
    "        for (_, s, e, is_fishing) in inferred:\n",
    "            s_min = datetime_to_minute(s)\n",
    "            e_min = datetime_to_minute(e)\n",
    "            for m in range(s_min - start_min, e_min - start_min + 1):\n",
    "                if 0 <= m < len(minutes):\n",
    "                    minutes[m, 1] = is_fishing       \n",
    " \n",
    "        mask = ((minutes[:, 0] != -1) & (minutes[:, 1] != -1))\n",
    "\n",
    "        if mask.sum():\n",
    "            true_by_mmsi[mmsi] = minutes[mask, 0]\n",
    "            pred_by_mmsi[mmsi] = minutes[mask, 1]\n",
    "            \n",
    "    return true_by_mmsi, pred_by_mmsi\n",
    "\n",
    "\n",
    "\n",
    "def ranges_from_ais(ais, is_fishing):\n",
    "    points = [trtools.Point(x['mmsi'], \n",
    "                            datetime.datetime.utcfromtimestamp(x['timestamp']).replace(tzinfo=pytz.utc), is_fishing[i])\n",
    "              for i, x in enumerate(ais)]\n",
    "    return trtools.ranges_from_points(points)\n",
    "\n",
    "def datetime_to_minute(dt):\n",
    "    timestamp = (dt - datetime.datetime(1970, 1, 1, tzinfo=pytz.utc)).total_seconds()\n",
    "    return int(timestamp // 60)\n",
    "\n",
    "\n",
    "def model_true_inferred(mdl, test_data):\n",
    "    check =  mdl.predict_proba(test_data)[:,1]\n",
    "    \n",
    "    inferred_ranges = list(ranges_from_ais(test_data, mdl.predict_proba(test_data)[:,1] > 0.5))\n",
    "    true_ranges = list(ranges_from_ais(test_data, test_data['classification'] > 0.5))\n",
    "    \n",
    "    true, inferred = compare_fishing_localization(true_ranges, inferred_ranges)\n",
    "    \n",
    "    y_true = np.concatenate(true.values())\n",
    "    y_pred = np.concatenate(inferred.values())\n",
    "    \n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "def model_metrics(y_true, y_pred):\n",
    "    \n",
    "    accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "    precision = metrics.precision_score(y_true, y_pred)\n",
    "    recall = metrics.recall_score(y_true, y_pred)\n",
    "    f1 = metrics.f1_score(y_true, y_pred)\n",
    "    \n",
    "    return precision, recall, accuracy, f1, y_true, y_pred\n",
    "    \n",
    "\n",
    "#     lines = [\"|Model|Recall|Precision|F1-Score|Accuracy|\",\n",
    "#          \"|-----|------|---------|--------|\"]\n",
    "\n",
    "\n",
    "#     for name in sorted(predictions):\n",
    "#         pred, actual = predictions[name]\n",
    "#         lines.append(\"|{}|{:.2f}|{:.2f}|{:.2f}|{:.2f}|\".format(\n",
    "#                 name, \n",
    "#                 metrics.recall_score(actual, pred),\n",
    "#                 metrics.precision_score(actual, pred), \n",
    "#                 metrics.f1_score(actual, pred),\n",
    "#                 metrics.accuracy_score(actual, pred)))\n",
    "#     return '\\n'.join(lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To get test data split by *predicted* class:\n",
    "import csv\n",
    "\n",
    "# Concatenate all test data\n",
    "all_data = np.concatenate([x[0] for x in rngdata.values()])\n",
    "\n",
    "# Create a mapping of true MMSI to class\n",
    "true_label_map = {}\n",
    "for gear_type in rngdata:\n",
    "    gear_name = os.path.splitext(os.path.splitext(gear_type)[0])[0][6:]\n",
    "    for x in rngdata[gear_type][0]:\n",
    "        true_label_map[x['mmsi']] = gear_name\n",
    "\n",
    "# Load a mapping of inferred MMSI to class\n",
    "inferred_label_map = {}\n",
    "with open(\"../../vessel-classification-pipeline copy/classification/inferred_labels.csv\") as f:\n",
    "    for row in csv.DictReader(f):\n",
    "        inferred_label_map[int(row['mmsi'])] = row['label'].strip().replace('/', '_').replace(' ', '_')\n",
    "\n",
    "testrngdata = {}\n",
    "    \n",
    "for gear_type in rngdata:\n",
    "    gear_name = os.path.splitext(os.path.splitext(gear_type)[0])[0][6:]\n",
    "    mask = [(inferred_label_map.get(int(x['mmsi'])) == gear_name and is_test(int(x['mmsi']))) for x in all_data]\n",
    "    mask = np.array(mask)\n",
    "    if mask.sum():\n",
    "        testrngdata[gear_type] = all_data[mask]\n",
    "\n",
    "# predict all results (below)\n",
    "\n",
    "# Split on true label map\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Longliners\n",
      "Computing Pole_and_Line\n",
      "Computing Pots_and_Traps\n",
      "Computing Purse_seines\n",
      "Computing Set_gillnets\n",
      "Computing Trawlers\n",
      "Computing Trollers\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>Random Forest with Distances</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "|Model|Count|Points|Precision|Recall|Accuracy|F1-Score|\n",
       "|-----|-----|------|---------|------|--------|--------|\n",
       "|Longliners|33|53089|0.85|0.79|0.91|0.82|\n",
       "|Pole_and_Line|3|3015|0.82|1.00|0.90|0.90|\n",
       "|Pots_and_Traps|3|1752|0.92|1.00|0.92|0.96|\n",
       "|Purse_seines|10|10686|0.72|1.00|0.87|0.84|\n",
       "|Set_gillnets|7|7803|0.65|1.00|0.87|0.79|\n",
       "|Trawlers|12|17915|0.92|0.96|0.92|0.94|\n",
       "|Trollers|1|2768|0.79|1.00|0.83|0.88|\n",
       "|     |     |     |     |     |\n",
       "|Overall|69|97028|0.84|0.91|0.90|0.87|"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = {\n",
    "#     'Random Forest' :vessel_scoring.random_forest_model.RandomForestModel(\n",
    "#         colspec=dict(windows=vessel_scoring.colspec.Colspec.windows,\n",
    "#                                     measures=['speed'])), \n",
    "        'Random Forest with Distances' :vessel_scoring.random_forest_model.RandomForestModel(\n",
    "                colspec=dict(windows=vessel_scoring.colspec.Colspec.windows,\n",
    "                                            measures=['speed', 'distance_from_shore', 'distance_from_port'])),\n",
    "#         'Logistic' :vessel_scoring.logistic_model.LogisticModel(order=6,\n",
    "#                 colspec=dict(windows=vessel_scoring.colspec.Colspec.windows,\n",
    "#                                             measures=['measure_speed'])),\n",
    "#         'Logistic with Distances' :vessel_scoring.logistic_model.LogisticModel(order=6,\n",
    "#                 colspec=dict(windows=vessel_scoring.colspec.Colspec.windows,\n",
    "#                                             measures=['measure_speed', 'measure_distance_from_shore', \n",
    "#     'measure_distance_from_port'])),                                                               \n",
    "#         'LogisticX' :vessel_scoring.logistic_model.LogisticModel(order=6, cross=4,\n",
    "#                 colspec=dict(windows=vessel_scoring.colspec.Colspec.windows,\n",
    "#                                             measures=['measure_speed'])),\n",
    "#         'LogisticX with Distances' :vessel_scoring.logistic_model.LogisticModel(order=6, cross=4,\n",
    "#                 colspec=dict(windows=vessel_scoring.colspec.Colspec.windows,\n",
    "#   measures=['measure_speed', 'measure_distance_from_shore', 'measure_distance_from_port'])),                                                         \n",
    "                                                               } \n",
    "\n",
    "lines = [\"|Model|Count|Points|Precision|Recall|Accuracy|F1-Score|\",\n",
    "         \"|-----|-----|------|---------|------|--------|--------|\"]\n",
    "\n",
    "all_true = []\n",
    "all_pred = []\n",
    "all_mmsi = []\n",
    "\n",
    "for gear in sorted(rngdata):\n",
    "    title = gear.split('.')[0].split('_',1)[1].replace('_', ' ')\n",
    "        \n",
    "    gear_name = os.path.splitext(os.path.splitext(gear)[0])[0][6:]\n",
    "    \n",
    "    print(\"Computing\", gear_name)\n",
    "    \n",
    "    all, _, _, _ = rngdata[gear]\n",
    "    all['classification'] = all['classification'] > 0.5\n",
    "\n",
    "    train  = []\n",
    "    test = []\n",
    "    for x in all:\n",
    "        if not is_test(int(x['mmsi'])):\n",
    "            train.append(x)\n",
    "    train = np.array(train)\n",
    "    if gear not in testrngdata:\n",
    "        continue\n",
    "    test = testrngdata[gear]\n",
    "    test['classification'] = test['classification'] > 0.5\n",
    "    \n",
    "    trained_models = []\n",
    "    for name, mdl in reversed(sorted(models.items())):\n",
    "        trained_models.append((name,\n",
    "                           vessel_scoring.models.train_model_on_data(mdl, train)))\n",
    "\n",
    "    name, mdl = trained_models[0]\n",
    "    y_true, y_pred =  model_true_inferred(mdl, test)\n",
    "\n",
    "    all_mmsi.append(test['mmsi'].astype(int))\n",
    "    all_true.append(y_true)\n",
    "    all_pred.append(y_pred)\n",
    "    \n",
    "    \n",
    "mmsi_all = np.concatenate(all_mmsi)\n",
    "y_true_all = np.concatenate(all_true)\n",
    "y_pred_all = np.concatenate(all_pred)\n",
    "\n",
    "for gear_type in sorted(set(true_label_map.values())):\n",
    "    mask = [(true_label_map.get(x) == gear_type and is_test(x)) for x in mmsi_all]\n",
    "    mask = np.array(mask)\n",
    "    y_true = y_true_all[mask]\n",
    "    y_pred = y_pred_all[mask]\n",
    "    mmsi_count = len(set(mmsi_all[mask]))\n",
    "    n_points = len(mmsi_all[mask])\n",
    "    accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "    precision = metrics.precision_score(y_true, y_pred)\n",
    "    recall = metrics.recall_score(y_true, y_pred)\n",
    "    f1 = metrics.f1_score(y_true, y_pred)\n",
    "    lines.append('|{}|{}|{}|'.format(gear_type, mmsi_count, n_points) + '|'.join(['{:.2f}'.format(x) for x in\n",
    "                                [precision, recall, accuracy, f1]]) + '|')\n",
    "    \n",
    "    \n",
    "mask = np.array([(is_test(x)) for x in mmsi_all])\n",
    "y_true = y_true_all[mask]\n",
    "y_pred = y_pred_all[mask]\n",
    "    \n",
    "mmsi_count =len(set(mmsi_all[mask]))\n",
    "n_points = len(mmsi_all[mask])\n",
    "accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "precision = metrics.precision_score(y_true, y_pred)\n",
    "recall = metrics.recall_score(y_true, y_pred)\n",
    "f1 = metrics.f1_score(y_true, y_pred)\n",
    "\n",
    "lines.append(\"|     |     |     |     |     |\")\n",
    "\n",
    "lines.append('|Overall|{}|{}|'.format(mmsi_count, n_points) + '|'.join(['{:.2f}'.format(x) for x in\n",
    "                            [precision, recall, accuracy, f1]]) + '|')\n",
    "\n",
    "display(HTML(\"<h2>{}</h2>\".format(name)))\n",
    "\n",
    "display(Markdown('\\n'.join(lines)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.923687074465\n"
     ]
    }
   ],
   "source": [
    "print((y_true_all == y_pred_all).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>Neural Net</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div class=\"unbreakable\">\n",
       "  <table>\n",
       "    <tr>\n",
       "      <th>\n",
       "        Gear Type\n",
       "      </th>\n",
       "      <th>\n",
       "        Precision\n",
       "      </th>\n",
       "      <th>\n",
       "        Recall\n",
       "      </th>\n",
       "      <th>\n",
       "        Accuracy\n",
       "      </th>\n",
       "      <th>\n",
       "        F1-Score\n",
       "      </th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\n",
       "        Longliners\n",
       "      </td>\n",
       "      <td>\n",
       "        0.85\n",
       "      </td>\n",
       "      <td>\n",
       "        0.82\n",
       "      </td>\n",
       "      <td>\n",
       "        0.85\n",
       "      </td>\n",
       "      <td>\n",
       "        0.83\n",
       "      </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\n",
       "        Pole and Line\n",
       "      </td>\n",
       "      <td>\n",
       "        0.91\n",
       "      </td>\n",
       "      <td>\n",
       "        0.95\n",
       "      </td>\n",
       "      <td>\n",
       "        0.96\n",
       "      </td>\n",
       "      <td>\n",
       "        0.93\n",
       "      </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\n",
       "        Pots and Traps\n",
       "      </td>\n",
       "      <td>\n",
       "        0.98\n",
       "      </td>\n",
       "      <td>\n",
       "        0.97\n",
       "      </td>\n",
       "      <td>\n",
       "        0.98\n",
       "      </td>\n",
       "      <td>\n",
       "        0.97\n",
       "      </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\n",
       "        Purse seines\n",
       "      </td>\n",
       "      <td>\n",
       "        0.83\n",
       "      </td>\n",
       "      <td>\n",
       "        0.87\n",
       "      </td>\n",
       "      <td>\n",
       "        0.90\n",
       "      </td>\n",
       "      <td>\n",
       "        0.85\n",
       "      </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\n",
       "        Set gillnets\n",
       "      </td>\n",
       "      <td>\n",
       "        0.97\n",
       "      </td>\n",
       "      <td>\n",
       "        0.86\n",
       "      </td>\n",
       "      <td>\n",
       "        0.93\n",
       "      </td>\n",
       "      <td>\n",
       "        0.91\n",
       "      </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\n",
       "        Trawlers\n",
       "      </td>\n",
       "      <td>\n",
       "        0.94\n",
       "      </td>\n",
       "      <td>\n",
       "        0.72\n",
       "      </td>\n",
       "      <td>\n",
       "        0.87\n",
       "      </td>\n",
       "      <td>\n",
       "        0.81\n",
       "      </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\n",
       "        Trollers\n",
       "      </td>\n",
       "      <td>\n",
       "        0.94\n",
       "      </td>\n",
       "      <td>\n",
       "        0.52\n",
       "      </td>\n",
       "      <td>\n",
       "        0.74\n",
       "      </td>\n",
       "      <td>\n",
       "        0.67\n",
       "      </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\n",
       "        Overall\n",
       "      </td>\n",
       "      <td>\n",
       "        0.88\n",
       "      </td>\n",
       "      <td>\n",
       "        0.80\n",
       "      </td>\n",
       "      <td>\n",
       "        0.87\n",
       "      </td>\n",
       "      <td>\n",
       "        0.84\n",
       "      </td>\n",
       "    </tr>\n",
       "  </table>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(\"<h2>Neural Net</h2>\"))\n",
    "display(HTML(fishing_localization))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Longliners\n",
      "Computing Pole_and_Line\n",
      "Computing Pots_and_Traps\n",
      "Computing Purse_seines\n",
      "Computing Set_gillnets\n",
      "Computing Trawlers\n",
      "Computing Trollers\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>Logistic</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "|Model|Count|Points|Precision|Recall|Accuracy|F1-Score|\n",
       "|-----|-----|------|---------|------|--------|--------|\n",
       "|Longliners|33|53089|0.80|0.75|0.89|0.78|\n",
       "|Pole_and_Line|3|3015|0.80|1.00|0.87|0.89|\n",
       "|Pots_and_Traps|3|1752|0.91|1.00|0.91|0.95|\n",
       "|Purse_seines|10|10686|0.76|1.00|0.87|0.86|\n",
       "|Set_gillnets|7|7803|0.50|1.00|0.80|0.67|\n",
       "|Trawlers|12|17915|0.86|0.93|0.87|0.89|\n",
       "|Trollers|1|2768|0.79|1.00|0.79|0.88|\n",
       "|     |     |     |     |     |\n",
       "|Overall|69|97028|0.79|0.88|0.87|0.84|"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = {\n",
    "#     'Random Forest' :vessel_scoring.random_forest_model.RandomForestModel(\n",
    "#         colspec=dict(windows=vessel_scoring.colspec.Colspec.windows,\n",
    "#                                     measures=['speed'])), \n",
    "#         'Random Forest with Distances' :vessel_scoring.random_forest_model.RandomForestModel(\n",
    "#                 colspec=dict(windows=vessel_scoring.colspec.Colspec.windows,\n",
    "#                                             measures=['speed', 'distance_from_shore', 'distance_from_port'])),\n",
    "        'Logistic' :vessel_scoring.logistic_model.LogisticModel(order=6,\n",
    "                colspec=dict(windows=vessel_scoring.colspec.Colspec.windows,\n",
    "                                            measures=['measure_speed'])),\n",
    "#         'Logistic with Distances' :vessel_scoring.logistic_model.LogisticModel(order=6,\n",
    "#                 colspec=dict(windows=vessel_scoring.colspec.Colspec.windows,\n",
    "#                                             measures=['measure_speed', 'measure_distance_from_shore', \n",
    "#     'measure_distance_from_port'])),                                                               \n",
    "#         'LogisticX' :vessel_scoring.logistic_model.LogisticModel(order=6, cross=4,\n",
    "#                 colspec=dict(windows=vessel_scoring.colspec.Colspec.windows,\n",
    "#                                             measures=['measure_speed'])),\n",
    "#         'LogisticX with Distances' :vessel_scoring.logistic_model.LogisticModel(order=6, cross=4,\n",
    "#                 colspec=dict(windows=vessel_scoring.colspec.Colspec.windows,\n",
    "#   measures=['measure_speed', 'measure_distance_from_shore', 'measure_distance_from_port'])),                                                         \n",
    "                                                               } \n",
    "\n",
    "lines = [\"|Model|Count|Points|Precision|Recall|Accuracy|F1-Score|\",\n",
    "         \"|-----|-----|------|---------|------|--------|--------|\"]\n",
    "\n",
    "all_true = []\n",
    "all_pred = []\n",
    "all_mmsi = []\n",
    "\n",
    "for gear in sorted(rngdata):\n",
    "    title = gear.split('.')[0].split('_',1)[1].replace('_', ' ')\n",
    "        \n",
    "    gear_name = os.path.splitext(os.path.splitext(gear)[0])[0][6:]\n",
    "    \n",
    "    print(\"Computing\", gear_name)\n",
    "    \n",
    "    all, _, _, _ = rngdata[gear]\n",
    "    all['classification'] = all['classification'] > 0.5\n",
    "\n",
    "    train  = []\n",
    "    test = []\n",
    "    for x in all:\n",
    "        if not is_test(int(x['mmsi'])):\n",
    "            train.append(x)\n",
    "    train = np.array(train)\n",
    "    if gear not in testrngdata:\n",
    "        continue\n",
    "    test = testrngdata[gear]\n",
    "    test['classification'] = test['classification'] > 0.5\n",
    "    \n",
    "    trained_models = []\n",
    "    for name, mdl in reversed(sorted(models.items())):\n",
    "        trained_models.append((name,\n",
    "                           vessel_scoring.models.train_model_on_data(mdl, train)))\n",
    "\n",
    "    name, mdl = trained_models[0]\n",
    "    y_true, y_pred =  model_true_inferred(mdl, test)\n",
    "\n",
    "    all_mmsi.append(test['mmsi'].astype(int))\n",
    "    all_true.append(y_true)\n",
    "    all_pred.append(y_pred)\n",
    "    \n",
    "    \n",
    "mmsi_all = np.concatenate(all_mmsi)\n",
    "y_true_all = np.concatenate(all_true)\n",
    "y_pred_all = np.concatenate(all_pred)\n",
    "\n",
    "for gear_type in sorted(set(true_label_map.values())):\n",
    "    mask = [(true_label_map.get(x) == gear_type and is_test(x)) for x in mmsi_all]\n",
    "    mask = np.array(mask)\n",
    "    y_true = y_true_all[mask]\n",
    "    y_pred = y_pred_all[mask]\n",
    "    mmsi_count = len(set(mmsi_all[mask]))\n",
    "    n_points = len(mmsi_all[mask])\n",
    "    accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "    precision = metrics.precision_score(y_true, y_pred)\n",
    "    recall = metrics.recall_score(y_true, y_pred)\n",
    "    f1 = metrics.f1_score(y_true, y_pred)\n",
    "    lines.append('|{}|{}|{}|'.format(gear_type, mmsi_count, n_points) + '|'.join(['{:.2f}'.format(x) for x in\n",
    "                                [precision, recall, accuracy, f1]]) + '|')\n",
    "    \n",
    "    \n",
    "mask = np.array([(is_test(x)) for x in mmsi_all])\n",
    "y_true = y_true_all[mask]\n",
    "y_pred = y_pred_all[mask]\n",
    "    \n",
    "mmsi_count =len(set(mmsi_all[mask]))\n",
    "n_points = len(mmsi_all[mask])\n",
    "accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "precision = metrics.precision_score(y_true, y_pred)\n",
    "recall = metrics.recall_score(y_true, y_pred)\n",
    "f1 = metrics.f1_score(y_true, y_pred)\n",
    "\n",
    "lines.append(\"|     |     |     |     |     |\")\n",
    "\n",
    "lines.append('|Overall|{}|{}|'.format(mmsi_count, n_points) + '|'.join(['{:.2f}'.format(x) for x in\n",
    "                            [precision, recall, accuracy, f1]]) + '|')\n",
    "\n",
    "display(HTML(\"<h2>{}</h2>\".format(name)))\n",
    "\n",
    "display(Markdown('\\n'.join(lines)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
