{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Models\n",
    "\n",
    "Compares the heursitic model that was previously used and the logistic model that is \n",
    "currently used.  Two different versions of the logistic model are shown. The vanilla\n",
    "model, which is what is currently in production and the multi-window model, that looks\n",
    "at features at multiple window sizes, that is likely to replace the vanilla model in the\n",
    "near future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model History\n",
    "\n",
    "### 1. Heuristic Model\n",
    "\n",
    "This model was derived by observing that there were correlations between fishing\n",
    "behaviour and several of the values present in AIS messages. In particular, the \n",
    "likelihood that a vessel was fishing tends to increase with the standard deviation of the speed ($\\sigma_s$) and course ($\\sigma_c$), but to decrease with mean speed. From this came the heursitic model:\n",
    "$$\n",
    "fishing\\_score = \\frac{2}{3}\\left(\\sigma_s + \\sigma_c + \\overline{\\mathcal{S}}\\right) \\\\\n",
    "\\overline{\\mathcal{S}} = \\text{mean}\\left(1.0 - \\min\\left(1, speed/17\\right)\\right)\n",
    "$$\n",
    "where the means and standard deviations are computed over a one hour window.\n",
    "\n",
    "The heuristic model performs reasonably well trawlers and longliners, but poorly for purse seiners.\n",
    "\n",
    "### 2. Logistic Regression Models\n",
    "\n",
    "A series of logistic regression models were then developed using the same three features\n",
    "found in the *heursitic model*. In order to increase the expressiveness of the logistic model,\n",
    "powers of the 3 base features are added to the features. Thus, the full feature vector consits of:\n",
    "$$\n",
    "\\sigma_s, {\\sigma_s}^2,\\ldots, {\\sigma_s}^n, \\sigma_c, {\\sigma_c}^2,\\ldots, {\\sigma_c}^n,\n",
    "\\overline{\\mathcal{S}},{\\overline{\\mathcal{S}}}^2, \\ldots {\\overline{\\mathcal{S}}}^n \\\\\n",
    "$$\n",
    "\n",
    "Note that that despite the odd form of \\overline{\\mathcal{S}}, from the point of view of the\n",
    "logistic model, it's equivalent to the the mean of the speed capped at 17 knots \n",
    "($\\text{mean}(\\min(speed, 17))$).\n",
    "\n",
    "#### Current Model\n",
    "\n",
    "The current model is a logistic model using a single window of 12 hours with $n=6$. One model is\n",
    "trained for all gear types. This model generally performs bettter than the heuristic model, but\n",
    "still performs ratherpoorly on purse seiners.\n",
    "\n",
    "####  Models in Progress\n",
    "\n",
    "The next model, on the verge of being deployed, uses multiple time windows, ranging in duration\n",
    "from one-half to twenty four hours, in order to have a richer feature set. In addition separate \n",
    "models are trained for each of the three primary gear types: longliners, trawlers and purse seines.\n",
    "We are also experimenting with adding some other features. In particular, whether it is currently \n",
    "daylight appears to be a very useful feature for predicting purse seine fishing. These changes, taken\n",
    "together, dramatically improve the performance, particularly of purse seiners.\n",
    "\n",
    "\n",
    "### 3. Future Model Types\n",
    "\n",
    "It is straightforward to convert the multi-window logistic model described above to a random forest or\n",
    "neural net model. In early experiments, both of these model types offer slightly  performance \n",
    "relative to logistic model while at the same eliminating the need to augment the feature vector with \n",
    "powers of the base features.\n",
    "\n",
    "We eventually plan to experiment with using convolutional or recurrent neural networks to find features\n",
    "in the AIS data directly rather than hand engineering the features.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from vessel_scoring import data\n",
    "from vessel_scoring.models import train_model_on_data\n",
    "from vessel_scoring.evaluate_model import compare_auc, compare_metrics\n",
    "from IPython.core.display import display, HTML, Markdown\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load training and test data\n",
    "_, train_lline,  valid_lline, test_lline = data.load_dataset_by_vessel(\n",
    "        'datasets/kristina_longliner.measures.npz')\n",
    "_, train_trawl,  valid_trawl, test_trawl = data.load_dataset_by_vessel(\n",
    "        'datasets/kristina_trawl.measures.npz')\n",
    "_, train_pseine, valid_pseine, test_pseine = data.load_dataset_by_vessel(\n",
    "        'datasets/kristina_ps.measures.npz')\n",
    "\n",
    "test_lline_crowd, _, _, _ = data.load_dataset_by_vessel(\n",
    "        \"datasets/classified-filtered.measures.npz\")\n",
    "\n",
    "train = np.concatenate([train_trawl, train_lline, train_pseine, valid_lline, valid_trawl, valid_pseine])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How much test data do we have\n",
    "\n",
    "Our initial test and training data consisted of roughly a dozen different vessels of each type \n",
    "classified over a multi-year period by Kristina Boerder of Dalhousie University. One-Quarter of \n",
    "those are used for testing, so there is a relatively small number of different vessels in the test\n",
    "sets. \n",
    "\n",
    "In addition, we are beginning to collect crowd sourced data for both testing and training. Some of the\n",
    "early crowd sourced data, available for long liners only, is used as an additional test set in the examples\n",
    "below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For trawlers we have 3 test vessels with 5000 test points\n",
      "For purse seiners we have 3 test vessels with 5000 test points\n",
      "For longliners we have 2 test vessels with 5000 test points\n",
      "For crowd sourced longliners we have 118 test vessels with 324166 test points\n"
     ]
    }
   ],
   "source": [
    "for name, test_data in [(\"trawlers\", test_trawl),\n",
    "                        (\"purse seiners\", test_pseine),\n",
    "                        (\"longliners\", test_lline),\n",
    "                        (\"crowd sourced longliners\", test_lline_crowd)]:\n",
    "    mmsi_count = len(set(test_data['mmsi']))\n",
    "    pt_count = len(test_data)\n",
    "    print(\"For {0} we have {1} test vessels with {2} test points\".format(name, mmsi_count, pt_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'windows'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a81256049888>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m          uniform_training_data),\n\u001b[0;32m     19\u001b[0m     ('Logistic', LogisticModel(windows=[43200],\n\u001b[1;32m---> 20\u001b[1;33m                                     order=6),\n\u001b[0m\u001b[0;32m     21\u001b[0m          uniform_training_data),\n\u001b[0;32m     22\u001b[0m     ('Logistic (MW)', LogisticModel(windows=[1800, 3600, 10800, 21600, 43200, 86400],\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'windows'"
     ]
    }
   ],
   "source": [
    "# Prepare the models\n",
    "\n",
    "from vessel_scoring.legacy_heuristic_model import LegacyHeuristicModel\n",
    "from vessel_scoring.logistic_model import LogisticModel\n",
    "\n",
    "uniform_training_data = {'longliner': train, \n",
    "                         'longliner crowd' : train,\n",
    "                         'trawler': train, \n",
    "                         'purse seiner': train}\n",
    "\n",
    "test_data = {'longliner': test_lline, \n",
    "             'longliner crowd': test_lline_crowd,\n",
    "             'trawler': test_trawl, \n",
    "             'purse seiner': test_pseine}\n",
    "\n",
    "untrained_models = [\n",
    "    (\"Legacy\", LegacyHeuristicModel(window=3600), \n",
    "         uniform_training_data),\n",
    "    ('Logistic', LogisticModel(windows=[43200],\n",
    "                                    order=6),\n",
    "         uniform_training_data),\n",
    "    ('Logistic (MW)', LogisticModel(windows=[1800, 3600, 10800, 21600, 43200, 86400],\n",
    "                                    order=6), \n",
    "         uniform_training_data),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Comparisons\n",
    "\n",
    "The models output a numbers between 0 and 1 that correspond to how \n",
    "confident they are that there is fishing occuring. For\n",
    "this first set of comparisons we treat predictions `>0.5`\n",
    "as fishing and those `<=0.5` as nonfishing. This allows us to use\n",
    "*precision*, *recall* and *f1-score* as metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import imp, vessel_scoring.evaluate_model; imp.reload(vessel_scoring.evaluate_model)\n",
    "from vessel_scoring.evaluate_model import (train_model, compare_pr,\n",
    "                                           compare_metrics, compare_metrics_table)\n",
    "\n",
    "for vessel_class in [\"longliner\", \"longliner crowd\", \"trawler\", \"purse seiner\"]:\n",
    "    display(HTML(\"<h3>Comparison for {0}</h3>\".format(vessel_class)))\n",
    "    models = []\n",
    "    for name, mdl, train_data in untrained_models:\n",
    "        models.append((name, train_model(mdl, train_data[vessel_class])))\n",
    "    display(Markdown(compare_metrics_table(models, test_data[vessel_class])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision - Recall Comparisons\n",
    "\n",
    "One way to compare the models without picking a specific threshold is\n",
    "to plot the precision versus recall of each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for vessel_class in [\"longliner\", \"longliner crowd\", \"trawler\", \"purse seiner\"]:\n",
    "    display(HTML(\"<h3>Comparison for {0}</h3>\".format(vessel_class)))\n",
    "    models = []\n",
    "    for name, mdl, train_data in untrained_models:\n",
    "        models.append((name, train_model(mdl, train_data[vessel_class])))\n",
    "    compare_pr(models, test_data[vessel_class])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Comparisons\n",
    "\n",
    "Another approach to compare continuous output is the Receiver Operator Characteristic curve.\n",
    "This curve plots *true positive rate* versus *false positive rate* and is useful for evaluating\n",
    "what is possible with different threshold values.  The Area Under the Curve (AUC) is used as \n",
    "a metric in this case, with a larger AUC being better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for vessel_class in [\"longliner\", \"longliner crowd\", \"trawler\", \"purse seiner\"]:\n",
    "    display(HTML(\"<h3>Comparison for {0}</h3>\".format(vessel_class)))\n",
    "    models = []\n",
    "    for name, mdl, train_data in untrained_models:\n",
    "        models.append((name, train_model(mdl, train_data[vessel_class])))\n",
    "    compare_auc(models, test_data[vessel_class])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
