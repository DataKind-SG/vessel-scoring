{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine sensitivity of current best model (Gear-Specific, Multi-Window \n",
    "Logistic Model with is-daylight) with respect to the various random\n",
    "seeds that are used during training.\n",
    "\n",
    "1. Random seed for model itself. Expect minimal effect\n",
    "\n",
    "2. Random seed for splitting training / testing data. Perhaps larger effect\n",
    "\n",
    "As it turns out (see below), neither makes much of a difference. Since this\n",
    "test was originally convceived the MMSI selection became a lot less random\n",
    "in order to better match the number of test and training MMSI. As a result,\n",
    "this lack of sensitivity may not hold as we add new MMSI. That said, we can\n",
    "expect the results to improve as we add more MMSI going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from vessel_scoring import data, utils\n",
    "from vessel_scoring.models import train_model_on_data\n",
    "from vessel_scoring.evaluate_model import evaluate_model, compare_models\n",
    "from IPython.core.display import display, HTML, Markdown\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vessel_scoring.logistic_model import LogisticModel\n",
    "\n",
    "def make_model(seed=4321):\n",
    "    return LogisticModel(colspec=dict(\n",
    "        windows=[1800, 3600, 10800, 21600, 43200, 86400],\n",
    "        measures=['measure_daylight', 'measure_speed']), order=6, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(seed=4321):\n",
    "    # Data supplied by Kristina\n",
    "    _, train_lline,  valid_lline, test_lline = data.load_dataset_by_vessel(\n",
    "            '../datasets/kristina_longliner.measures.npz', seed)\n",
    "    _, train_trawl,  valid_trawl, test_trawl = data.load_dataset_by_vessel(\n",
    "            '../datasets/kristina_trawl.measures.npz', seed)\n",
    "    _, train_pseine, valid_pseine, test_pseine = data.load_dataset_by_vessel(\n",
    "            '../datasets/kristina_ps.measures.npz', seed)\n",
    "\n",
    "    # Slow transits (used to train models to avoid classifying slow transits as fishing)\n",
    "    TRANSIT_WEIGHT = 10\n",
    "    x_tran, xtrain_tran, xcross_tran, xtest_tran = data.load_dataset_by_vessel(\n",
    "                                        '../datasets/slow-transits.measures.npz', even_split=False, seed=seed)\n",
    "    xtrain_tran = utils.clone_subset(xtrain_tran, test_lline.dtype)\n",
    "    xcross_tran = utils.clone_subset(xcross_tran, test_lline.dtype)\n",
    "    xtest_tran = utils.clone_subset(xtest_tran, test_lline.dtype)\n",
    "    train_tran = np.concatenate([xtrain_tran, xcross_tran] * TRANSIT_WEIGHT)\n",
    "\n",
    "    train = {'longliner': np.concatenate([train_lline, valid_lline, train_tran]), \n",
    "                'trawler': np.concatenate([train_trawl, valid_trawl, train_tran]),\n",
    "                'purse_seine': np.concatenate([train_pseine, valid_pseine, train_tran])}\n",
    "    \n",
    "    test = {'longliner': test_lline, \n",
    "            'trawler': test_trawl, \n",
    "            'purse_seine': test_pseine}\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_seeds(count):\n",
    "    np.random.seed(4321)\n",
    "    return np.random.randint(4294967295, size=count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First investigate sensitivity of the LogisiticModels to different seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N_SEEDS = 10\n",
    "\n",
    "train_data, test_data = load_data()\n",
    "\n",
    "for gear in ['purse_seine', 'trawler', 'longliner']:\n",
    "    X_test = test_data[gear]\n",
    "\n",
    "    display(HTML(\"<h2>{}</h2>\".format(gear.replace('_', ' ').title())))\n",
    "        \n",
    "    predictions = []\n",
    "    trained_models = []\n",
    "    for seed in get_seeds(N_SEEDS):\n",
    "        mdl = make_model(seed)\n",
    "        \n",
    "        train_model_on_data(mdl, train_data[gear])\n",
    "        \n",
    "        trained_models.append((seed, mdl))\n",
    "        \n",
    "        predictions.append((seed, (mdl.predict_proba(X_test)[:,1] > 0.5), X_test['classification'] > 0.5))\n",
    "\n",
    "    lines = [\"|Model|Recall|Precision|F1-Score|\",\n",
    "         \"|-----|------|---------|--------|\"]\n",
    "    for name, pred, actual in predictions:\n",
    "        lines.append(\"|{}|{:.2f}|{:.2f}|{:.2f}|\".format(name, \n",
    "                                            metrics.recall_score(actual, pred),\n",
    "                                            metrics.precision_score(actual, pred), \n",
    "                                            metrics.f1_score(actual, pred)))\n",
    "\n",
    "    display(Markdown('\\n'.join(lines)))\n",
    "\n",
    "    compare_models(trained_models, X_test)\n",
    "    \n",
    "    display(HTML(\"<hr/>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially no difference when setting the seed for different runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about setting the seed when loading the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for gear in ['purse_seine', 'trawler', 'longliner']:\n",
    "    X_test = test_data[gear]\n",
    "\n",
    "    display(HTML(\"<h2>{}</h2>\".format(gear.replace('_', ' ').title())))\n",
    "        \n",
    "    predictions = []\n",
    "    trained_models = []\n",
    "    for seed in get_seeds(N_SEEDS):\n",
    "        mdl = make_model()\n",
    "        \n",
    "        train_data, test_data = load_data(seed)\n",
    "        \n",
    "        train_model_on_data(mdl, train_data[gear])\n",
    "        \n",
    "        trained_models.append((seed, mdl))\n",
    "        \n",
    "        predictions.append((seed, (mdl.predict_proba(X_test)[:,1] > 0.5), X_test['classification'] > 0.5))\n",
    "\n",
    "    lines = [\"|Model|Recall|Precision|F1-Score|\",\n",
    "         \"|-----|------|---------|--------|\"]\n",
    "    for name, pred, actual in predictions:\n",
    "        lines.append(\"|{}|{:.2f}|{:.2f}|{:.2f}|\".format(name, \n",
    "                                            metrics.recall_score(actual, pred),\n",
    "                                            metrics.precision_score(actual, pred), \n",
    "                                            metrics.f1_score(actual, pred)))\n",
    "\n",
    "    display(Markdown('\\n'.join(lines)))\n",
    "\n",
    "    compare_models(trained_models, X_test)\n",
    "    \n",
    "    display(HTML(\"<hr/>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance this looks impressive, but in retrospect the \n",
    "new code for dividing up up MMSI evenly no longer allow much randomness.\n",
    "So this may not mean as much as I originally thought it would."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
