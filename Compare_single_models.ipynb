{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare models trained on mixed class data\n",
    "\n",
    "This compares several different models trained on **mixed** *longliner*,\n",
    "*trawler* and *purse seiner* data.\n",
    "\n",
    "**NOTE: this was somewhat unstable. Running multiple times yielded significantly different results, \n",
    "  depending the data split.  I set seeds everywhere and now the output is stable. However, this tells \n",
    "  me that we want more data!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from vessel_scoring import data\n",
    "from vessel_scoring.evaluate_model import evaluate_model, train_model, compare_models\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning, insufficient items to sample, returning all\n",
      "Warning, inufficient items to sample, returning 3168\n"
     ]
    }
   ],
   "source": [
    "x_tran, xtrain_tran, xcross_tran, xtest_tran = data.load_dataset_by_vessel('datasets/slow-transits.measures.npz',\n",
    "                                                                     even_split=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(x_tran['mmsi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_, xtrain_trawl, xcross_trawl, xtest_trawl = data.load_dataset_by_vessel('datasets/kristina_trawl.measures.npz')\n",
    "_, xtrain_lline, xcross_lline, xtest_lline = data.load_dataset_by_vessel('datasets/kristina_longliner.measures.npz')\n",
    "_, xtrain_pseine, xcross_pseine, xtest_pseine = data.load_dataset_by_vessel('datasets/kristina_ps.measures.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clone_subset(x, dtype):\n",
    "    \"\"\"copy only the portions of x in dtype to a new array\"\"\"\n",
    "    new = np.zeros(x.shape, dtype=dtype)\n",
    "    for name in dtype.names:\n",
    "        new[name] = x[name]\n",
    "    return new\n",
    "        \n",
    "# We need to make the fields in the tran data match that of the other \n",
    "# to concatenate\n",
    "xtrain_tran = clone_subset(xtrain_tran, xtrain_lline.dtype)\n",
    "xcross_tran = clone_subset(xcross_tran, xtrain_lline.dtype)\n",
    "xtest_tran = clone_subset(xtest_tran, xtrain_lline.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRANSIT_WEIGHT = 10\n",
    "\n",
    "xtrain = np.concatenate([xtrain_trawl, xtrain_lline, xtrain_pseine] + [xtrain_tran] * TRANSIT_WEIGHT)\n",
    "xcross = np.concatenate([xcross_trawl, xcross_lline, xcross_pseine] + [xcross_tran] * TRANSIT_WEIGHT)\n",
    "\n",
    "train = np.concatenate([xtrain, xcross])\n",
    "\n",
    "xtest = np.concatenate([xtest_trawl, xtest_lline, xtest_pseine, xtest_tran])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from vessel_scoring.legacy_heuristic_model import LegacyHeuristicModel\n",
    "from vessel_scoring.random_forest_model import RandomForestModel\n",
    "from vessel_scoring.logistic_model import LogisticModel\n",
    "\n",
    "untrained_models = [\n",
    "    ('Logistic', LogisticModel(windows=[43200], order=6)),\n",
    "    ('Logistic opt MSE', LogisticModel(windows=[43200], order=4, cross=3)),\n",
    "#     ('Logistic (MW)', LogisticModel(windows=[1800, 3600, 10800, 21600, 43200, 86400], order=6)),\n",
    "#     ('Logistic (MW/cross3)', LogisticModel(windows=[1800, 3600, 10800, 21600, 43200, 86400], order=6, cross=2)),\n",
    "    ('Random Forest', RandomForestModel(windows=[43200])),\n",
    "#     ('Random Forest (MW)', RandomForestModel(windows=[1800, 3600, 10800, 21600, 43200, 86400])),\n",
    "#     ('Legacy', LegacyHeuristicModel(window=3600)),\n",
    "#     (\"Legacy (3 Hour)\", LegacyHeuristicModel(window=10800)),\n",
    "#     (\"Legacy (12 Hour)\", LegacyHeuristicModel(window=43200)),\n",
    "#     (\"Legacy (24 Hour)\", LegacyHeuristicModel(window=86400)),  \n",
    "]\n",
    "\n",
    "trained_models = [(name, train_model(mdl, train)) for (name, mdl) in untrained_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Overall Comparison</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'trained_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-1b1224d0c1ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHTML\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"<h1>Overall Comparison</h1>\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmdl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrained_models\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mspec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvessel_scoring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muntrained_models\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trained_models' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "display(HTML(\"<h1>Overall Comparison</h1>\"))\n",
    "for name, mdl in trained_models:\n",
    "    spec = vessel_scoring.models.untrained_models[name]\n",
    "\n",
    "    test_data = [dataset[name]['test'] for name in spec['data']]\n",
    "    test_data = vessel_scoring.utils.concatenate_different_recarrays(test_data)   \n",
    "\n",
    "    evaluate_model(mdl, test_data, name=name)\n",
    "\n",
    "display(HTML(\"<h1>Overall comparison</h1>\"))\n",
    "compare_models(trained_models, xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for vessel_class, xtest_class in  [(\"longliner\", xtest_lline), \n",
    "                                   (\"trawler\", xtest_trawl), \n",
    "                                   (\"purse seine\", xtest_pseine)]:\n",
    "    display(HTML(\"<h1>Comparison for {0}</h1>\".format(vessel_class)))\n",
    "    for name, mdl in trained_models:\n",
    "        evaluate_model(mdl, xtest_class, name=name)\n",
    "    display(HTML(\"<h1>Comparison for {0}</h1>\".format(vessel_class)))\n",
    "    compare_models(trained_models, xtest_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does this model do on the slow transit tracks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for name, mdl in trained_models:\n",
    "    p = mdl.predict_proba(xtest_tran)[:,1]\n",
    "    print name, p.mean(), (p > 0.5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about the new transit tracks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from vessel_scoring import data\n",
    "x_tran2 = np.load('datasets/new_transits.measures.npz')['x']\n",
    "\n",
    "# We are somehow getting an extra, 0 MMSI. Why?\n",
    "# Results are totally bogus\n",
    "x_tran2 = x_tran2[x_tran2['mmsi'] != 0]\n",
    "\n",
    "print \"Transit tracks for\", sorted(set(x_tran2['mmsi']))\n",
    "assert not (set(x_tran2['mmsi']) & set(x_tran['mmsi'])), \"overlap between old and new transit sets\"\n",
    "print\n",
    "for name, mdl in trained_models:\n",
    "    p = mdl.predict_proba(x_tran2)[:,1]\n",
    "    print name, p.mean(), (p > 0.5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where are the false positives?\n",
    "\n",
    "The important thing is that the false positives are not evenly spaced out, so they shouldn't\n",
    "show up as track like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "mmsi = sorted(set(x_tran2['mmsi']))\n",
    "\n",
    "name, mdl = trained_models[1]\n",
    "\n",
    "for m in mmsi:\n",
    "    subset = x_tran2[x_tran2['mmsi'] == m]\n",
    "    n = len(subset)\n",
    "    locs = np.arange(n)[mdl.predict_proba(subset)[:,1] > 0.5]\n",
    "    print \"FP rate for MMSI\", int(m), \"is:\", len(locs) / float(n)\n",
    "    plt.figure()\n",
    "    plt.title(\"MMSI {0}\".format(m))\n",
    "    plt.hist(locs, bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify that we get same results when training using saved models\n",
    "\n",
    "When loading from saved models we still get the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from vessel_scoring.logistic_model import LogisticScorer\n",
    "from glob import glob\n",
    "import json\n",
    "paths = glob(\"vessel_scoring/models/*.json\")\n",
    "\n",
    "print \"Transit tracks for\", sorted(set(x_tran2['mmsi']))\n",
    "assert not (set(x_tran2['mmsi']) & set(x_tran['mmsi'])), \"overlap between old and new transit sets\"\n",
    "print\n",
    "for pth in paths:\n",
    "    with open(pth) as f:\n",
    "        args = json.load(f)['args']['args']\n",
    "    mdl = LogisticScorer(**args)\n",
    "    p = mdl.predict_proba(x_tran2)[:,1]\n",
    "    print pth, p.mean(), (p > 0.5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results when not training with transit data.\n",
    "\n",
    "    Logistic 0.838618506196 0.984949092519\n",
    "    Logistic opt MSE 0.826260344662 0.986277113767\n",
    "    Random Forest 0.718120849934 0.885790172643\n",
    "    Legacy 0.623862134036 0.773351040283\n",
    "    Legacy (12 Hour) 0.66843170167 0.912793271359\n",
    "    \n",
    "**If we don't train on these slow transit tracks, results are pretty awful**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dumping the model and using LogisticScorer\n",
    "\n",
    "LogisticScorer is a simple reimplementation of the prediciton part of \n",
    "the logistic predictor. This way we can dump out the parameters from \n",
    "the model and then use it in the pipeline where we potentially can\n",
    "optimize it for that particular application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from vessel_scoring.logistic_model import LogisticScorer\n",
    "\n",
    "scorer = LogisticScorer(**trained_models[1][1].dump_dict())\n",
    "\n",
    "print scorer.fishing_score(xtest).max()\n",
    "print scorer.predict_proba(xtest)[:,1].max()\n",
    "print trained_models[1][1].predict_proba(xtest)[:,1].max()\n",
    "\n",
    "evaluate_model(scorer, xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Verify that we can also load LogisticModels from dumped LogisticModels\n",
    "# Had to abuse sklearn interface a bit for this to work\n",
    "import imp, vessel_scoring.logistic_model\n",
    "imp.reload(vessel_scoring.logistic_model)\n",
    "\n",
    "\n",
    "from vessel_scoring.logistic_model import LogisticModel\n",
    "\n",
    "\n",
    "model = LogisticModel(**trained_models[1][1].dump_dict())\n",
    "\n",
    "print model.predict_proba(xtest)[:,1].max()\n",
    "print trained_models[1][1].predict_proba(xtest)[:,1].max()\n",
    "\n",
    "evaluate_model(model, xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "trained_models[1][1].dump_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load existing models and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import vessel_scoring.models\n",
    "loaded_models = vessel_scoring.models.load_models()\n",
    "dataset = vessel_scoring.models.load_data()\n",
    "\n",
    "display(HTML(\"<h1>Overall Comparison</h1>\"))\n",
    "for name, mdl in loaded_models.iteritems():\n",
    "    if '--' not in name:\n",
    "        continue\n",
    "        \n",
    "    spec = vessel_scoring.models.untrained_models[name]\n",
    "\n",
    "    test_data = [dataset[dataname]['test'] for dataname in spec['data']]\n",
    "    test_data = vessel_scoring.utils.concatenate_different_recarrays(test_data)   \n",
    "\n",
    "    method, dataname = name.split(\"--\")\n",
    "        \n",
    "    evaluate_model(mdl, test_data, name=\"%s trained and tested on %s\" % (method, dataname))\n",
    "\n",
    "    generic_model = loaded_models[name.split(\"--\")[0]]\n",
    "\n",
    "    evaluate_model(generic_model, test_data, name=\"Generic %s tested on \" % (method, dataname))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
