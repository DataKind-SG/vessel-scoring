{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Descriptions\n",
    "------------------\n",
    "\n",
    "\n",
    "## Heuristic Model\n",
    "\n",
    "The first model developed is referred to as the\n",
    "*heuristic model* and was derived by observing that there were\n",
    "correlations between fishing behaviour and several of the\n",
    "values present in AIS messages. In particular, the\n",
    "likelihood that a vessel was fishing tends to increase with\n",
    "the standard deviation of the speed ($\\sigma_s$) and course\n",
    "($\\sigma_c$), but to decrease with mean speed. These features\n",
    "were used to develop the *heursitic model*:\n",
    "$$\n",
    "fishing\\_score = \\frac{2}{3}\\left(\\sigma_{s_m} + \\sigma_{c_m} + \\overline{s_m}\\right) \\\\\n",
    "s_m = 1.0 - \\min\\left(1, speed\\,/\\,17\\right) \\\\\n",
    "c_m = course\\,/\\,360 \\\\\n",
    "$$\n",
    "where the means and standard deviations are computed over a one hour window.\n",
    "\n",
    "The heuristic model performs reasonably well trawlers and\n",
    "longliners, but poorly for purse seiners.\n",
    "\n",
    "ADD TAG/REVISION INFO TO SEE SPECIFIC CODE.\n",
    "\n",
    "\n",
    "## Generic Model\n",
    "\n",
    "A series of logistic regression models were then developed\n",
    "using the same three features found in the *heursitic\n",
    "model*. In order to increase the expressiveness of the\n",
    "logistic model, powers of the 3 base features are added to\n",
    "the features. Thus, the full feature vector consits of:\n",
    "$$\n",
    "\\sigma_{s_m}, \\sigma_{s_m}^2,\\ldots, \\sigma_{s_m}^n, \\sigma_{c_m}, \\sigma_{c_m}^2,\\ldots, \\sigma_{c_m}^n,\n",
    "\\overline{s_m},\\overline{s_m}^2, \\ldots \\overline{s_m}^n \\\\\n",
    "$$\n",
    "where $n$ is what we shall be refer to as the *feature order*.\n",
    "Note that that despite the odd form of\n",
    "$s_m$, from the point of view of the\n",
    "logistic model, it's equivalent to the the speed\n",
    "capped at 17 knots.\n",
    "\n",
    "The first of the logistic models, referred to as the\n",
    "*generic model*, is the model currently in use and \n",
    "is a logistic model using a 12 hour time window\n",
    "a feature order of 6. One model is trained for all gear\n",
    "types. This model generally performs bettter than the\n",
    "heuristic model, but still performs rather poorly on purse\n",
    "seiners. The 12-hour window was arrived at by plotting the model\n",
    "accuracy versus window size. There is a different optimal\n",
    "window size for each gear type, but 12 hours performed\n",
    "well for a model trained and tested on all gear types.\n",
    "\n",
    "ADD TAG/REVISION INFO TO SEE SPECIFIC CODE.\n",
    "\n",
    "## Multi-Window Model\n",
    "\n",
    "The multi-window model, which is on the verge of being deployed, \n",
    "is a logistic model similar to the *generic model* except that\n",
    "is use uses\n",
    "multiple time windows, ranging in duration from one-half to\n",
    "twenty four hours. Using multiple window sizes both provides\n",
    "a richer feature set and avoids the needs to optimize over\n",
    "window size. In\n",
    "addition separate models are trained for each of the three\n",
    "primary gear types: longliners, trawlers and purse seines.\n",
    "We are also experimenting with adding other features.\n",
    "In particular, whether it is currently daylight appears to\n",
    "be a very useful feature for predicting purse seine fishing.\n",
    "These changes, taken together, dramatically improve the\n",
    "performance, particularly of purse seiners.\n",
    "\n",
    "\n",
    "## Future Models\n",
    "\n",
    "It is straightforward to use the multi-window logistic\n",
    "model features described above with a random forest or neural net\n",
    "model. In early experiments, both of these model types offer\n",
    "slightly improved performance relative to logistic model while at\n",
    "the same eliminating the need to augment the feature vector\n",
    "with powers of the base features.\n",
    "\n",
    "We eventually plan to experiment with using convolutional or\n",
    "recurrent neural networks to find features in the AIS data\n",
    "directly rather than hand engineering the features.\n",
    "\n",
    "\n",
    "General Notes\n",
    "-------------\n",
    "\n",
    "The precision of the models vary by gear type: Long liners are easiest to\n",
    "predict, even for a model trained on all gear types,\n",
    "followed by trawlers; purse seiners are the worst.\n",
    "\n",
    "We have evaluated the models using a separate test set (and\n",
    "for window size and feature order, optimization, using\n",
    "separate train-, validation- and test-sets) plotting\n",
    "precision/recall and ROC curves.\n",
    "\n",
    "We have also evaluated the generic model on each gear type\n",
    "separately as well as on the combined data set. In addition,\n",
    "for longliners we have cross trained and validated between\n",
    "two separately labelled datasets with slightly different\n",
    "labeling methods (Kristinas' and Alex data).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Models\n",
    "\n",
    "Compares the heursitic model that was previously used and the logistic model that is \n",
    "currently used.  Two different versions of the logistic model are shown. The vanilla\n",
    "model, which is what is currently in production and the multi-window model, that looks\n",
    "at features at multiple window sizes, that is likely to replace the vanilla model in the\n",
    "near future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from vessel_scoring import data\n",
    "from vessel_scoring.models import train_model_on_data\n",
    "from vessel_scoring.evaluate_model import compare_auc, compare_metrics\n",
    "from IPython.core.display import display, HTML, Markdown\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load training and test data\n",
    "_, train_lline,  valid_lline, test_lline = data.load_dataset_by_vessel(\n",
    "        'datasets/kristina_longliner.measures.npz')\n",
    "_, train_trawl,  valid_trawl, test_trawl = data.load_dataset_by_vessel(\n",
    "        'datasets/kristina_trawl.measures.npz')\n",
    "_, train_pseine, valid_pseine, test_pseine = data.load_dataset_by_vessel(\n",
    "        'datasets/kristina_ps.measures.npz')\n",
    "\n",
    "test_lline_crowd, _, _, _ = data.load_dataset_by_vessel(\n",
    "        \"datasets/classified-filtered.measures.npz\")\n",
    "\n",
    "train = np.concatenate([train_trawl, train_lline, train_pseine, valid_lline, valid_trawl, valid_pseine])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How much test data do we have\n",
    "\n",
    "Our initial test and training data consisted of roughly a dozen different vessels of each type \n",
    "classified over a multi-year period by Kristina Boerder of Dalhousie University. One-Quarter of \n",
    "those are used for testing, so there is a relatively small number of different vessels in the test\n",
    "sets. \n",
    "\n",
    "In addition, we are beginning to collect crowd sourced data for both testing and training. Some of the\n",
    "early crowd sourced data, available for long liners only, is used as an additional test set in the examples\n",
    "below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For trawlers we have 3 test vessels with 5000 test points\n",
      "For purse seiners we have 3 test vessels with 5000 test points\n",
      "For longliners we have 2 test vessels with 5000 test points\n",
      "For crowd sourced longliners we have 118 test vessels with 324166 test points\n"
     ]
    }
   ],
   "source": [
    "for name, test_data in [(\"trawlers\", test_trawl),\n",
    "                        (\"purse seiners\", test_pseine),\n",
    "                        (\"longliners\", test_lline),\n",
    "                        (\"crowd sourced longliners\", test_lline_crowd)]:\n",
    "    mmsi_count = len(set(test_data['mmsi']))\n",
    "    pt_count = len(test_data)\n",
    "    print(\"For {0} we have {1} test vessels with {2} test points\".format(name, mmsi_count, pt_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a81256049888>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvessel_scoring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogistic_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m uniform_training_data = {'longliner': train, \n\u001b[0m\u001b[1;32m      7\u001b[0m                          \u001b[0;34m'longliner crowd'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                          \u001b[0;34m'trawler'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "# Prepare the models\n",
    "\n",
    "from vessel_scoring.legacy_heuristic_model import LegacyHeuristicModel\n",
    "from vessel_scoring.logistic_model import LogisticModel\n",
    "\n",
    "uniform_training_data = {'longliner': train, \n",
    "                         'longliner crowd' : train,\n",
    "                         'trawler': train, \n",
    "                         'purse seiner': train}\n",
    "\n",
    "test_data = {'longliner': test_lline, \n",
    "             'longliner crowd': test_lline_crowd,\n",
    "             'trawler': test_trawl, \n",
    "             'purse seiner': test_pseine}\n",
    "\n",
    "untrained_models = [\n",
    "    (\"Legacy\", LegacyHeuristicModel(window=3600), \n",
    "         uniform_training_data),\n",
    "    ('Logistic', LogisticModel(windows=[43200],\n",
    "                                    order=6),\n",
    "         uniform_training_data),\n",
    "    ('Logistic (MW)', LogisticModel(windows=[1800, 3600, 10800, 21600, 43200, 86400],\n",
    "                                    order=6), \n",
    "         uniform_training_data),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Comparisons\n",
    "\n",
    "The models output a numbers between 0 and 1 that correspond to how \n",
    "confident they are that there is fishing occuring. For\n",
    "this first set of comparisons we treat predictions `>0.5`\n",
    "as fishing and those `<=0.5` as nonfishing. This allows us to use\n",
    "*precision*, *recall* and *f1-score* as metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import imp, vessel_scoring.evaluate_model; imp.reload(vessel_scoring.evaluate_model)\n",
    "from vessel_scoring.evaluate_model import (train_model, compare_pr,\n",
    "                                           compare_metrics, compare_metrics_table)\n",
    "\n",
    "for vessel_class in [\"longliner\", \"longliner crowd\", \"trawler\", \"purse seiner\"]:\n",
    "    display(HTML(\"<h3>Comparison for {0}</h3>\".format(vessel_class)))\n",
    "    models = []\n",
    "    for name, mdl, train_data in untrained_models:\n",
    "        models.append((name, train_model(mdl, train_data[vessel_class])))\n",
    "    display(Markdown(compare_metrics_table(models, test_data[vessel_class])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision - Recall Comparisons\n",
    "\n",
    "One way to compare the models without picking a specific threshold is\n",
    "to plot the precision versus recall of each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for vessel_class in [\"longliner\", \"longliner crowd\", \"trawler\", \"purse seiner\"]:\n",
    "    display(HTML(\"<h3>Comparison for {0}</h3>\".format(vessel_class)))\n",
    "    models = []\n",
    "    for name, mdl, train_data in untrained_models:\n",
    "        models.append((name, train_model(mdl, train_data[vessel_class])))\n",
    "    compare_pr(models, test_data[vessel_class])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Comparisons\n",
    "\n",
    "Another approach to compare continuous output is the Receiver Operator Characteristic curve.\n",
    "This curve plots *true positive rate* versus *false positive rate* and is useful for evaluating\n",
    "what is possible with different threshold values.  The Area Under the Curve (AUC) is used as \n",
    "a metric in this case, with a larger AUC being better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for vessel_class in [\"longliner\", \"longliner crowd\", \"trawler\", \"purse seiner\"]:\n",
    "    display(HTML(\"<h3>Comparison for {0}</h3>\".format(vessel_class)))\n",
    "    models = []\n",
    "    for name, mdl, train_data in untrained_models:\n",
    "        models.append((name, train_model(mdl, train_data[vessel_class])))\n",
    "    compare_auc(models, test_data[vessel_class])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
