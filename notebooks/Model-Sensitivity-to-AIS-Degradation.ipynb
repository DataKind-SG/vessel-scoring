{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine sensitivity of current best model (Gear-Specific, Multi-Window \n",
    "Logistic Model with is-daylight) with respect to degredation of the AIS\n",
    "data. \n",
    "\n",
    "We train as normal, then progessively randomly dropout more and more of\n",
    "the training data and see how that effects the resulting reported\n",
    "accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from vessel_scoring import data, utils\n",
    "from vessel_scoring.models import train_model_on_data\n",
    "from vessel_scoring.evaluate_model import evaluate_model, compare_models\n",
    "from IPython.core.display import display, HTML, Markdown\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vessel_scoring.logistic_model import LogisticModel\n",
    "\n",
    "def make_model(seed=4321):\n",
    "    return LogisticModel(colspec=dict(\n",
    "        windows=[1800, 3600, 10800, 21600, 43200, 86400],\n",
    "        measures=['measure_daylight', 'measure_speed']), order=6, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(suffix='', seed=4321):\n",
    "    # Data supplied by Kristina\n",
    "    _, train_lline,  valid_lline, test_lline = data.load_dataset_by_vessel(\n",
    "            '../datasets/kristina_longliner.measures{}.npz'.format(suffix), seed)\n",
    "    _, train_trawl,  valid_trawl, test_trawl = data.load_dataset_by_vessel(\n",
    "            '../datasets/kristina_trawl.measures{}.npz'.format(suffix), seed)\n",
    "    _, train_pseine, valid_pseine, test_pseine = data.load_dataset_by_vessel(\n",
    "            '../datasets/kristina_ps.measures{}.npz'.format(suffix), seed)\n",
    "\n",
    "    # Slow transits (used to train models to avoid classifying slow transits as fishing)\n",
    "    TRANSIT_WEIGHT = 10\n",
    "    x_tran, xtrain_tran, xcross_tran, xtest_tran = data.load_dataset_by_vessel(\n",
    "                        '../datasets/slow-transits.measures{}.npz'.format(suffix), even_split=False, seed=seed)\n",
    "    xtrain_tran = utils.clone_subset(xtrain_tran, test_lline.dtype)\n",
    "    xcross_tran = utils.clone_subset(xcross_tran, test_lline.dtype)\n",
    "    xtest_tran = utils.clone_subset(xtest_tran, test_lline.dtype)\n",
    "    train_tran = np.concatenate([xtrain_tran, xcross_tran] * TRANSIT_WEIGHT)\n",
    "\n",
    "    train = {'longliner': np.concatenate([train_lline, valid_lline, train_tran]), \n",
    "                'trawler': np.concatenate([train_trawl, valid_trawl, train_tran]),\n",
    "                'purse_seine': np.concatenate([train_pseine, valid_pseine, train_tran])}\n",
    "    \n",
    "    test = {'longliner': test_lline, \n",
    "            'trawler': test_trawl, \n",
    "            'purse_seine': test_pseine}\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning, insufficient items to sample, returning all\n",
      "Warning, inufficient items to sample, returning 4430\n",
      "Warning, inufficient items to sample, returning 2084\n",
      "Warning, insufficient items to sample, returning all\n",
      "Warning, inufficient items to sample, returning 2244\n",
      "Warning, inufficient items to sample, returning 1084\n",
      "Warning, insufficient items to sample, returning all\n",
      "Warning, inufficient items to sample, returning 922\n",
      "Warning, inufficient items to sample, returning 453\n",
      "Warning, inufficient items to sample, returning fewer\n",
      "Warning, insufficient items to sample, returning all\n",
      "Warning, inufficient items to sample, returning 467\n",
      "Warning, inufficient items to sample, returning 225\n",
      "Warning, inufficient items to sample, returning fewer\n",
      "Warning, insufficient items to sample, returning all\n",
      "Warning, inufficient items to sample, returning 81\n",
      "Warning, inufficient items to sample, returning fewer\n",
      "Warning, insufficient items to sample, returning all\n",
      "Warning, inufficient items to sample, returning fewer\n",
      "Warning, insufficient items to sample, returning all\n",
      "Warning, inufficient items to sample, returning 24\n",
      "Warning, inufficient items to sample, returning fewer\n",
      "Warning, insufficient items to sample, returning all\n",
      "Warning, inufficient items to sample, returning 37\n",
      "Warning, insufficient items to sample, returning all\n",
      "Warning, inufficient items to sample, returning fewer\n",
      "Warning, inufficient items to sample, returning 665\n",
      "Warning, insufficient items to sample, returning all\n",
      "Warning, inufficient items to sample, returning fewer\n",
      "Warning, inufficient items to sample, returning 919\n",
      "Warning, insufficient items to sample, returning all\n",
      "Warning, inufficient items to sample, returning fewer\n",
      "Warning, insufficient items to sample, returning all\n",
      "Warning, inufficient items to sample, returning 12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>Purse Seine</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "|Model|Recall|Precision|F1-Score|ROC-AUC|\n",
       "|-----|------|---------|--------|--------|\n",
       "|1|0.72|0.21|0.33|0.93|\n",
       "|0.5|0.62|0.22|0.32|0.90|\n",
       "|0.2|0.55|0.17|0.26|0.89|\n",
       "|0.1|0.48|0.21|0.29|0.85|\n",
       "|0.05|0.44|0.17|0.25|0.88|\n",
       "|0.01|0.17|0.14|0.15|0.83|\n",
       "|0.02|0.15|0.10|0.12|0.82|\n",
       "|0.005|0.21|0.30|0.25|0.87|"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>Trawler</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "|Model|Recall|Precision|F1-Score|ROC-AUC|\n",
       "|-----|------|---------|--------|--------|\n",
       "|1|0.87|0.94|0.90|0.95|\n",
       "|0.5|0.81|0.94|0.87|0.94|\n",
       "|0.2|0.80|0.93|0.86|0.94|\n",
       "|0.1|0.73|0.95|0.83|0.93|\n",
       "|0.05|0.67|0.97|0.80|0.92|\n",
       "|0.01|0.36|0.94|0.52|0.87|\n",
       "|0.02|0.51|0.96|0.66|0.90|\n",
       "|0.005|0.23|0.95|0.37|0.88|"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>Longliner</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "|Model|Recall|Precision|F1-Score|ROC-AUC|\n",
       "|-----|------|---------|--------|--------|\n",
       "|1|0.90|0.94|0.92|0.95|\n",
       "|0.5|0.90|0.96|0.93|0.95|\n",
       "|0.2|0.87|0.96|0.92|0.94|\n",
       "|0.1|0.89|0.99|0.94|0.98|\n",
       "|0.05|0.79|0.97|0.87|0.94|\n",
       "|0.01|0.47|0.98|0.64|0.88|\n",
       "|0.02|0.64|0.97|0.77|0.91|\n",
       "|0.005|0.41|0.97|0.58|0.87|"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keep_probs = [1, 0.5, 0.2, 0.1, 0.05, 0.01, 0.02, 0.005]\n",
    "\n",
    "train_data, test_data = load_data()\n",
    "\n",
    "drop_test = {1: test_data}\n",
    "for kp in keep_probs[1:]:\n",
    "    suffix = '_{}'.format(str(kp).replace('.', ''))\n",
    "    drop_test[kp] = load_data(suffix=suffix)[1]\n",
    "\n",
    "for gear in ['purse_seine', 'trawler', 'longliner']:\n",
    "    mdl = make_model()\n",
    "    train_model_on_data(mdl, train_data[gear])\n",
    "    \n",
    "    display(HTML(\"<h2>{}</h2>\".format(gear.replace('_', ' ').title())))\n",
    "                \n",
    "    predictions = []\n",
    "    for kp in keep_probs:  \n",
    "\n",
    "        td = drop_test[kp][gear]\n",
    "            \n",
    "        raw = mdl.predict_proba(td)[:,1]\n",
    "        predictions.append((kp, raw, (raw > 0.5), td['classification'] > 0.5))\n",
    "\n",
    "    lines = [\"|Model|Recall|Precision|F1-Score|ROC-AUC|\",\n",
    "         \"|-----|------|---------|--------|--------|\"]\n",
    "    for name, raw, pred, actual in predictions:\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(actual, raw)\n",
    "        auc = metrics.auc(fpr, tpr)\n",
    "        lines.append(\"|{}|{:.2f}|{:.2f}|{:.2f}|{:.2f}|\".format(name, \n",
    "                                            metrics.recall_score(actual, pred),\n",
    "                                            metrics.precision_score(actual, pred), \n",
    "                                            metrics.f1_score(actual, pred),\n",
    "                                            auc))\n",
    "\n",
    "    display(Markdown('\\n'.join(lines)))    \n",
    "    display(HTML(\"<hr/>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
