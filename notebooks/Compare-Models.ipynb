{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Models\n",
    "\n",
    "This notebook compares various GFW models based on the `measure_speed` and `measure_course` with each other\n",
    "and with the models from Dalhousie University.  Note that the distance-to-shore cutoff was disabled in the\n",
    "Dalhousie models, so none of the models compared here are using distance-to-shore as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "%matplotlib inline\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append('..')\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "from IPython.core.display import display, HTML, Markdown\n",
    "import datetime\n",
    "import pytz\n",
    "from sklearn import metrics\n",
    "import csv\n",
    "import vessel_scoring.models\n",
    "import vessel_scoring.evaluate_model\n",
    "from vessel_scoring import utils\n",
    "import vessel_scoring.data\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "sys.path.append(\"../../training-data-source\")\n",
    "import tools as trtools\n",
    "sys.path.append(\"../../vessel-classification-pipeline/classification\")\n",
    "from  classification.utility import is_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning, insufficient items to sample, returning all\n",
      "Warning, insufficient items to sample, returning all\n"
     ]
    }
   ],
   "source": [
    "# Load the data. \n",
    "# ==============\n",
    "\n",
    "# *First load the load the split datasets generated from PyBossa data*\n",
    "\n",
    "paths = glob(\"../datasets/from_ranges/*.npz\")\n",
    "rngdata = {os.path.basename(p) : vessel_scoring.data.load_dataset_by_vessel(p) for p in paths\n",
    "          if p.endswith('.measures.npz')}\n",
    "\n",
    "# *Then load mappings MMSI to true / inferred gear type\n",
    "\n",
    "def gear_name_of(x):\n",
    "    return os.path.splitext(os.path.splitext(x)[0])[0][9:]\n",
    "\n",
    "# Create a mapping of MMSI to true label\n",
    "true_label_map = {}\n",
    "for gear_type in rngdata:\n",
    "    gear_name = gear_name_of(gear_type)\n",
    "    for x in rngdata[gear_type][0]:\n",
    "        true_label_map[x['mmsi']] = gear_name\n",
    "        \n",
    "# Create mapping of MMSI to inferred label\n",
    "inferred_label_map = {}\n",
    "with open(\"../../vessel-classification-pipeline copy/classification/inferred_labels.csv\") as f:\n",
    "    for row in csv.DictReader(f):\n",
    "        inferred_label_map[int(row['mmsi'])] = row['label'].strip().replace('/', '_').replace(' ', '_')\n",
    "        \n",
    "# *Create datasets for training and testing*\n",
    "\n",
    "# If this is true, test using inferred classes,\n",
    "# otherwise use true classes\n",
    "test_using_inferred = True\n",
    "\n",
    "all_data = np.concatenate([x[0] for x in rngdata.values()])\n",
    "\n",
    "test_data = {}\n",
    "train_data = {}\n",
    "all_data = np.concatenate([x[0] for x in rngdata.values()])\n",
    "all_data['classification'] = (all_data['classification'] > 0.5)\n",
    "\n",
    "for gear_type in rngdata:\n",
    "    gear_name = gear_name_of(gear_type)\n",
    "    \n",
    "    # use the true gear types for the training data\n",
    "    true_mask = [(true_label_map.get(int(x['mmsi'])) == gear_name and not is_test(int(x['mmsi']))) \n",
    "                     for x in all_data]\n",
    "    true_mask = np.array(true_mask)\n",
    "    if true_mask.sum():\n",
    "        train_data[gear_type] = all_data[true_mask]    \n",
    "\n",
    "    if test_using_inferred:\n",
    "        # Use the inferred gear types for the test data\n",
    "        test_mask = np.array(\n",
    "            [(inferred_label_map.get(int(x['mmsi'])) == gear_name and is_test(int(x['mmsi'])))\n",
    "                         for x in all_data])\n",
    "    else:\n",
    "        # Use the inferred gear types for the test data\n",
    "        test_mask = np.array(\n",
    "            [(true_label_map.get(int(x['mmsi'])) == gear_name and is_test(int(x['mmsi'])))\n",
    "                         for x in all_data])\n",
    "    if test_mask.sum():\n",
    "        test_data[gear_type] = all_data[test_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compare_fishing_localization(true_ranges, inferred_ranges):\n",
    "\n",
    "    all_mmsi = sorted(set(x.mmsi for x in true_ranges))\n",
    "\n",
    "    true_by_mmsi = {}\n",
    "    pred_by_mmsi = {}\n",
    "\n",
    "    for mmsi in all_mmsi:\n",
    "        true = np.array([x for x in true_ranges if x.mmsi == mmsi])\n",
    "        inferred = np.array([x for x in inferred_ranges if x.mmsi == mmsi])\n",
    "\n",
    "        # Determine minutes from start to finish of this mmsi, create an array to\n",
    "        # hold results and fill with -1 (unknown)\n",
    "        _, start, end, _ = true[0]\n",
    "        for (_, s, e, _) in true[1:]:\n",
    "            start = min(start, s)\n",
    "            end = max(end, e)\n",
    "        start_min = datetime_to_minute(start)\n",
    "        end_min = datetime_to_minute(end)\n",
    "        \n",
    "        minutes = np.empty([end_min - start_min + 1, 2], dtype=int)\n",
    "        minutes.fill(-1)\n",
    "\n",
    "        # Fill in minutes[:, 0] with known true / false values\n",
    "        for (_, s, e, is_fishing) in true:\n",
    "            s_min = datetime_to_minute(s)\n",
    "            e_min = datetime_to_minute(e)\n",
    "            for m in range(s_min - start_min, e_min - start_min + 1):\n",
    "                minutes[m, 0] = is_fishing\n",
    "\n",
    "        # fill in minutes[:, 1] with in inferred values\n",
    "        for (_, s, e, is_fishing) in inferred:\n",
    "            s_min = datetime_to_minute(s)\n",
    "            e_min = datetime_to_minute(e)\n",
    "            for m in range(s_min - start_min, e_min - start_min + 1):\n",
    "                if 0 <= m < len(minutes):\n",
    "                    minutes[m, 1] = is_fishing       \n",
    " \n",
    "        mask = ((minutes[:, 0] != -1) & (minutes[:, 1] != -1))\n",
    "\n",
    "        if mask.sum():\n",
    "            true_by_mmsi[mmsi] = minutes[mask, 0]\n",
    "            pred_by_mmsi[mmsi] = minutes[mask, 1]\n",
    "            \n",
    "    return true_by_mmsi, pred_by_mmsi\n",
    "\n",
    "def ranges_from_ais(ais, is_fishing):\n",
    "    points = [trtools.Point(x['mmsi'], \n",
    "                            datetime.datetime.utcfromtimestamp(x['timestamp']).replace(tzinfo=pytz.utc), is_fishing[i])\n",
    "              for i, x in enumerate(ais)]\n",
    "    return trtools.ranges_from_points(points)\n",
    "\n",
    "def datetime_to_minute(dt):\n",
    "    timestamp = (dt - datetime.datetime(1970, 1, 1, tzinfo=pytz.utc)).total_seconds()\n",
    "    return int(timestamp // 60)\n",
    "\n",
    "def model_true_inferred(mdl, test_data):\n",
    "    check =  mdl.predict_proba(test_data)[:,1]\n",
    "    \n",
    "    inferred_ranges = list(ranges_from_ais(test_data, mdl.predict_proba(test_data)[:,1] > 0.5))\n",
    "    true_ranges = list(ranges_from_ais(test_data, test_data['classification'] > 0.5))\n",
    "    \n",
    "    true, inferred = compare_fishing_localization(true_ranges, inferred_ranges)\n",
    "    \n",
    "    y_true = np.concatenate(true.values())\n",
    "    y_pred = np.concatenate(inferred.values())\n",
    "    \n",
    "    return y_true, y_pred\n",
    "\n",
    "def model_metrics(y_true, y_pred):\n",
    "    \n",
    "    accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "    precision = metrics.precision_score(y_true, y_pred)\n",
    "    recall = metrics.recall_score(y_true, y_pred)\n",
    "    f1 = metrics.f1_score(y_true, y_pred)\n",
    "    \n",
    "    return precision, recall, accuracy, f1, y_true, y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......."
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>Random Forest with Distances</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "|Model|Count|Points|Precision|Recall|Accuracy|F1-Score|\n",
       "|-----|-----|------|---------|------|--------|--------|\n",
       "|Longliners|40|122240|0.91|0.87|0.90|0.89|\n",
       "|Pole_and_Line|4|8197|0.82|0.93|0.82|0.87|\n",
       "|Pots_and_Traps|5|10082|0.73|0.99|0.88|0.84|\n",
       "|Purse_seines|15|17595|0.76|1.00|0.84|0.86|\n",
       "|Set_gillnets|8|11448|0.80|0.98|0.82|0.88|\n",
       "|Trawlers|15|33446|0.98|0.94|0.96|0.96|\n",
       "|Trollers|1|2768|1.00|0.72|0.72|0.84|\n",
       "|     |     |     |     |     |\n",
       "|Overall|88|205776|0.88|0.91|0.89|0.89|"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......."
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>Logistic</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "|Model|Count|Points|Precision|Recall|Accuracy|F1-Score|\n",
       "|-----|-----|------|---------|------|--------|--------|\n",
       "|Longliners|40|122240|0.89|0.93|0.92|0.91|\n",
       "|Pole_and_Line|4|8197|0.71|0.90|0.69|0.79|\n",
       "|Pots_and_Traps|5|10082|0.68|0.97|0.85|0.80|\n",
       "|Purse_seines|15|17595|0.76|1.00|0.84|0.86|\n",
       "|Set_gillnets|8|11448|0.77|0.99|0.79|0.87|\n",
       "|Trawlers|15|33446|0.92|0.94|0.92|0.93|\n",
       "|Trollers|1|2768|0.90|0.87|0.81|0.89|\n",
       "|     |     |     |     |     |\n",
       "|Overall|88|205776|0.85|0.94|0.89|0.90|"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = [\n",
    "        ('Random Forest with Distances', vessel_scoring.random_forest_model.RandomForestModel(\n",
    "                colspec=dict(windows=vessel_scoring.colspec.Colspec.windows,\n",
    "                                measures=['speed', 'distance_from_shore', 'distance_from_port']))),\n",
    "        ('Logistic', vessel_scoring.logistic_model.LogisticModel(order=6,\n",
    "                colspec=dict(windows=vessel_scoring.colspec.Colspec.windows,\n",
    "                                measures=['measure_speed']))),\n",
    "] \n",
    "\n",
    "for name, mdl in models:\n",
    "\n",
    "    lines = [\"|Model|Count|Points|Precision|Recall|Accuracy|F1-Score|\",\n",
    "             \"|-----|-----|------|---------|------|--------|--------|\"]\n",
    "\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "    all_mmsi = []\n",
    "\n",
    "    for gear in sorted(rngdata):\n",
    "        title = gear.split('.')[0].split('_',1)[1].replace('_', ' ')\n",
    "\n",
    "        gear_name = os.path.splitext(os.path.splitext(gear)[0])[0][9:]\n",
    "\n",
    "        print(\".\", end=\"\")\n",
    "        \n",
    "        if gear not in test_data:\n",
    "            continue\n",
    "        \n",
    "        trained = vessel_scoring.models.train_model_on_data(mdl, train_data[gear])\n",
    "\n",
    "        y_true, y_pred =  model_true_inferred(trained, test_data[gear])\n",
    "\n",
    "        all_mmsi.append(test_data[gear]['mmsi'].astype(int))\n",
    "        all_true.append(y_true)\n",
    "        all_pred.append(y_pred)\n",
    "\n",
    "        \n",
    "    # Now remap the data from the inferred labels to the true labels so that\n",
    "    # we can place them in the correct buckets\n",
    "\n",
    "    mmsi_all = np.concatenate(all_mmsi)\n",
    "    y_true_all = np.concatenate(all_true)\n",
    "    y_pred_all = np.concatenate(all_pred)\n",
    "    \n",
    "    for gear_type in sorted(set(true_label_map.values())):\n",
    "        # Create a mask of all points with MMSI corresponding to this TRUE label\n",
    "        mask = [(true_label_map.get(x) == gear_type and is_test(x)) for x in mmsi_all]\n",
    "        mask = np.array(mask)\n",
    "        # Fine y_true, y_pred based using the mask and compute metrics\n",
    "        y_true = y_true_all[mask]\n",
    "        y_pred = y_pred_all[mask]\n",
    "        mmsi_count = len(set(mmsi_all[mask]))\n",
    "        n_points = len(mmsi_all[mask])\n",
    "        accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "        precision = metrics.precision_score(y_true, y_pred)\n",
    "        recall = metrics.recall_score(y_true, y_pred)\n",
    "        f1 = metrics.f1_score(y_true, y_pred)\n",
    "        # Add to Markdown table\n",
    "        lines.append('|{}|{}|{}|'.format(gear_type, mmsi_count, n_points) + '|'.join(['{:.2f}'.format(x) for x in\n",
    "                                    [precision, recall, accuracy, f1]]) + '|')\n",
    "\n",
    "    # Compute metrics over all test points.\n",
    "    mask = np.array([(is_test(x)) for x in mmsi_all])\n",
    "    y_true = y_true_all[mask]\n",
    "    y_pred = y_pred_all[mask]\n",
    "\n",
    "    mmsi_count =len(set(mmsi_all[mask]))\n",
    "    n_points = len(mmsi_all[mask])\n",
    "    accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "    precision = metrics.precision_score(y_true, y_pred)\n",
    "    recall = metrics.recall_score(y_true, y_pred)\n",
    "    f1 = metrics.f1_score(y_true, y_pred)\n",
    "\n",
    "    lines.append(\"|     |     |     |     |     |\")\n",
    "\n",
    "    lines.append('|Overall|{}|{}|'.format(mmsi_count, n_points) + '|'.join(['{:.2f}'.format(x) for x in\n",
    "                                [precision, recall, accuracy, f1]]) + '|')\n",
    "\n",
    "    display(HTML(\"<h2>{}</h2>\".format(name)))\n",
    "\n",
    "    display(Markdown('\\n'.join(lines)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
